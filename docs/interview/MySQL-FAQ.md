---
title: MySQL 三万字精华总结 + 面试100 问，和面试官扯皮绰绰有余
date: 2024-05-31
tags: 
 - MySQL
categories: MySQL
---

![](https://img.starfish.ink/common/faq-banner.png)

> 写在之前：不建议那种上来就是各种面试题罗列，然后背书式的去记忆，对技术的提升帮助很小，对正经面试也没什么帮助，有点东西的面试官深挖下就懵逼了。
>
> 个人建议把面试题看作是费曼学习法中的回顾、简化的环节，准备面试的时候，跟着题目先自己讲给自己听，看看自己会满意吗，不满意就继续学习这个点，如此反复，心仪的offer肯定会有的。
>
> 当然，大家有遇到过什么样『有趣』『有含量』的题目，欢迎提出来，一起学习~

## 一、MySQL 架构

> 和其它数据库相比，MySQL 有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎的架构上，**插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离**。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。
>
> ![](https://img.starfish.ink/mysql/architecture.png)
>
> - **连接层**：最上层是一些客户端和连接服务。**主要完成一些类似于连接处理、授权认证、及相关的安全方案**。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。
>
> - **服务层**：第二层服务层，主要完成大部分的核心服务功能， 包括查询解析、分析、优化、缓存、以及所有的内置函数，所有跨存储引擎的功能也都在这一层实现，包括触发器、存储过程、视图等 
>
> - **引擎层**：第三层存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取
>
> - **存储层**：第四层为数据存储层，主要是将数据存储在运行于该设备的文件系统之上，并完成与存储引擎的交互
>



### MySQL 的查询流程具体是怎么样的？

> [!TIP]
>
> 一条 SQL 语句在 MySQL 中如何执行的？
>
> 画出 MySQL 架构图？ 「这种变态问题都能问的出来~」

1. **客户端请求**：客户端通过连接器发送查询到 MySQL 服务器（验证用户身份，给予权限）
2. **查询接收**：连接器接收请求，管理连接
3. **解析器**：对 SQL 进行词法分析和语法分析，转换为解析树
4. **优化器**：优化器生成执行计划，选择最优索引和连接顺序
5. **查询执行器**：执行器执行查询，通过存储引擎接口获取数据
6. **存储引擎**：存储引擎检索数据，返回给执行器
7. **返回结果**：结果通过连接器返回给客户端

![](https://img.starfish.ink/mysql/MySQL-select-flow.png)

------



## 二、存储引擎

> 存储引擎是 MySQL 的组件，用于处理不同表类型的 SQL 操作。不同的存储引擎提供不同的存储机制、索引技巧、锁定水平等功能，使用不同的存储引擎，还可以获得特定的功能。
>
> 使用哪一种引擎可以灵活选择，**<mark>一个数据库中多个表可以使用不同引擎以满足各种性能和实际需求</mark>**，使用合适的存储引擎，将会提高整个数据库的性能 。
>
>  MySQL 服务器使用**可插拔**的存储引擎体系结构，可以从运行中的 MySQL 服务器加载或卸载存储引擎 。
>
> **查看存储引擎**
>
> ```mysql
> -- 查看支持的存储引擎
> SHOW ENGINES
> 
> -- 查看默认存储引擎
> SHOW VARIABLES LIKE 'storage_engine'
> 
> --查看具体某一个表所使用的存储引擎，这个默认存储引擎被修改了！
> show create table tablename
> 
> --准确查看某个数据库中的某一表所使用的存储引擎
> show table status like 'tablename'
> show table status from database where name="tablename"
> ```
>
> **设置存储引擎**
>
> ```mysql
> -- 建表时指定存储引擎。默认的就是INNODB，不需要设置
> CREATE TABLE t1 (i INT) ENGINE = INNODB;
> CREATE TABLE t2 (i INT) ENGINE = CSV;
> CREATE TABLE t3 (i INT) ENGINE = MEMORY;
> 
> -- 修改存储引擎
> ALTER TABLE t ENGINE = InnoDB;
> 
> -- 修改默认存储引擎，也可以在配置文件my.cnf中修改默认引擎
> SET default_storage_engine=NDBCLUSTER;
> ```
>

### 说说 MySQL 都有哪些存储引擎？都有哪些区别？

常见的存储引擎就 InnoDB、MyISAM、Memory、NDB。

InnoDB 现在是 MySQL5.5 版本后默认的存储引擎，支持**事务、行级锁定和外键**。我们一般和 MyISM 进行对比即可

1. **事务支持**：InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
2. **外键约束**：InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败；  
3. **存储结构**：InnoDB 是聚簇索引，MyISAM 是非聚簇索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 
4.  **表空间管理**：InnoDB 使用表空间（tablespace）来管理数据存储，支持自动扩展，支持表和索引分开存储，提高存储效率。MyISAM 每个表有三个文件：`.frm`（表定义文件）、`.MYD`（数据文件）和 `.MYI`（索引文件）
5. **锁定机制**：InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
6. **崩溃恢复**：InnoDB 具有崩溃恢复的能力，使用重做日志（redo log）和回滚日志（undo log）来恢复数据；MyISAM 没有崩溃恢复机制，可能需要手动恢复
7. **性能**： InnoDB 在写密集型操作中表现更好，特别是在需要事务和外键约束的场景下。MyISAM 在读密集型操作中表现更好，尤其是在没有写操作的情况下。

 

> #### 文件存储结构对比
>
> 在 MySQL中建立任何一张数据表，在其数据目录对应的数据库目录下都有对应表的 `.frm` 文件，`.frm` 文件是用来保存每个数据表的元数据(meta)信息，包括表结构的定义等，与数据库存储引擎无关，也就是任何存储引擎的数据表都必须有`.frm`文件，命名方式为 数据表名.frm，如user.frm。
>
> 查看 MySQL 数据保存在哪里：`show variables like 'data%'`
>
> MyISAM 物理文件结构为：
>
> - `.frm`文件：与表相关的元数据信息都存放在frm文件，包括表结构的定义信息等
> - `.MYD` (`MYData`) 文件：MyISAM 存储引擎专用，用于存储 MyISAM 表的数据
> - `.MYI` (`MYIndex`)文件：MyISAM 存储引擎专用，用于存储 MyISAM 表的索引相关信息
>
> InnoDB 物理文件结构为：
>
> - `.frm` 文件：与表相关的元数据信息都存放在 frm 文件，包括表结构的定义信息等
>
> - `.ibd` 文件或 `.ibdata` 文件： 这两种文件都是存放 InnoDB 数据的文件，之所以有两种文件形式存放 InnoDB 的数据，是因为 InnoDB 的数据存储方式能够通过配置来决定是使用**共享表空间**存放存储数据，还是用**独享表空间**存放存储数据。
>
>   独享表空间存储方式使用`.ibd`文件，并且每个表一个`.ibd`文件
>   共享表空间存储方式使用`.ibdata`文件，所有表共同使用一个`.ibdata`文件（或多个，可自己配置）
>



### 哪个存储引擎执行 select count(*) 更快，为什么?

MyISAM更快，因为MyISAM内部维护了一个计数器，可以直接调取。

- MyISAM 存储每个表的行数在表的元数据中，因此执行 `SELECT COUNT(*)` 时，它可以直接读取这个值，而不需要扫描整个表

- nnoDB 不存储行数信息在表的元数据中。每次执行 `SELECT COUNT(*)` 查询时，InnoDB 都需要扫描整个表来计算行数。这对于大表来说可能会非常慢。

InnoDB 不将表的行数存储在元数据中，主要原因是其设计目标与 MyISAM 不同。InnoDB 设计为支持高并发的事务处理和数据一致性，因此其存储和计数机制需要权衡性能和一致性。以下是一些具体原因：

**1. 行级锁定和并发控制**

InnoDB 支持行级锁定，这意味着在高并发环境中，不同事务可以同时对不同的行进行操作，而不会相互阻塞。为了确保这种并发控制和数据一致性，InnoDB 需要动态计算行数，以反映当前事务视图下的数据状态。

- **事务隔离级别**：InnoDB 支持多种事务隔离级别（如 READ COMMITTED、REPEATABLE READ、SERIALIZABLE），这些隔离级别决定了事务如何看到数据。预先存储的行数无法满足这些隔离级别的要求，因为行数在不同事务下可能有所不同。
- **锁机制**：由于行级锁定，InnoDB 在处理大量并发事务时，需要动态调整行数信息，而不是依赖预先存储的静态行数。

**2. 一致性和持久性**

InnoDB 设计为支持 ACID 属性（原子性、一致性、隔离性、持久性），这要求所有的数据操作都必须保证一致性和可靠性。

- **崩溃恢复**：InnoDB 使用重做日志（redo log）和回滚日志（undo log）来实现崩溃恢复。如果行数保存在元数据中，崩溃恢复后行数可能与实际数据不一致，从而破坏数据的一致性。
- **并发更新**：在高并发环境中，多个事务可能同时修改表中的数据。如果行数保存在元数据中，每次更新都需要锁定并更新元数据，这将导致严重的性能瓶颈。

**3. 性能优化**

动态计算行数虽然在某些查询中（如 `SELECT COUNT(*)`）较慢，但它避免了在高并发写操作下频繁更新元数据的性能开销。

- **写操作性能**：为了保证高效的写操作，InnoDB 设计避免了每次写操作都需要更新元数据的设计，这样可以更好地处理高并发写入。
- **实际应用**：在实际应用中，行数的精确统计并不是经常需要的操作。大多数情况下，应用程序可以通过索引和其他机制来实现高效的数据访问，而不依赖于 `SELECT COUNT(*)` 的性能。

------



## 三、数据类型

> 主要包括以下五大类：
>
> - 整数类型：BIT、BOOL、TINY INT、SMALL INT、MEDIUM INT、 INT、 BIG INT
> - 浮点数类型：FLOAT、DOUBLE、DECIMAL
> - 字符串类型：CHAR、VARCHAR、TINY TEXT、TEXT、MEDIUM TEXT、LONGTEXT、TINY BLOB、BLOB、MEDIUM BLOB、LONG BLOB
> - 日期类型：Date、DateTime、TimeStamp、Time、Year
> - 其他数据类型：BINARY、VARBINARY、ENUM、SET、Geometry、Point、MultiPoint、LineString、MultiLineString、Polygon、GeometryCollection等



### CHAR 和 VARCHAR 的区别？

`CHAR` 和 `VARCHAR` 都是用于存储字符串的字段类型，但它们在存储方式、性能和使用场景上存在一些关键区别

- **存储方式**：char 是固定长度，varchar长度可变：

  - char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30) 就可以存储 30 个字符。

  - 存储时，char 不管实际存储数据的长度，直接按 char 规定的长度分配存储空间；而 varchar 会根据实际存储的数据分配最终的存储空间，加上 1 或 2 个额外字节用于存储字符串的长度信息

- **性能不同**：

  - `CHAR` 因为其固定长度的特性，在某些情况下可能会提供更好的性能，尤其是当处理大量具有相同长度的数据时。
  - `VARCHAR` 由于需要存储长度信息，并且可能需要额外的空间来存储不同长度的字符串，因此在性能上可能略逊于 `CHAR`

- 使用场景：char 适用于固定长度字符串：如存储状态码、国家代码、MD5 哈希值等。varchar 适用于可变长度字符串：如用户名、电子邮件地址、描述等。



### MySQL 里记录货币用什么字段类型比较好？

> 阿里规范：【强制】任何货币金额，均以最小货币单位且整型类型来进行存储。

在 MySQL 中记录货币数据时，通常推荐使用 `DECIMAL` 类型。`DECIMAL` 类型提供高精度的存储和计算，非常适合用于存储货币值。以下是使用 `DECIMAL` 类型的原因以及其他可能选择的字段类型和其适用场景：

**使用 `DECIMAL` 类型的原因**

1. **高精度**：
   - `DECIMAL` 类型可以精确存储货币值，没有浮点运算误差。例如，定义为 `DECIMAL(10, 2)` 表示最多 10 位数字，其中 2 位小数，适合存储最多到亿位的金额，精确到小数点后两位。
2. **存储效率**：
   - 由于货币值通常需要精确到小数点后两位，`DECIMAL` 能够确保存储的每一个值都是精确的，避免了浮点数可能带来的舍入误差。
3. **计算正确性**：
   - 在涉及到财务计算时，使用浮点数类型（如 `FLOAT` 或 `DOUBLE`）可能会因为舍入误差导致计算不准确。`DECIMAL` 类型避免了这些问题，确保计算结果的准确性。

当然还**有些业务**，用 `INT` 或者 `BIGINT` 类型存储货币的最小单位也可以（如美分、分），适合对性能有更高要求的场景（整数运算比浮点运算更快），但需要处理转换逻辑。



> 这个问题其实很不重要，因为大部分公司不让让使用这两种类型。
>
> **禁止使用TEXT、BLOB类型**：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能

### BLOB和TEXT有什么区别？

`BLOB`（Binary Large Object）和 `TEXT` 是 MySQL 中用于存储大型二进制数据和大型文本数据的两种不同的数据类型。它们之间的主要区别包括：

1. **存储内容**：`BLOB` 用于存储二进制数据，如图片、音频、视频等。`TEXT` 用于存储大型文本数据，如文章、评论等。
2. **最大长度**：
   - `BLOB` 和 `TEXT` 类型的最大长度可以达到 65,535 字节，即 64KB（在 MySQL 5.0.3 之前的版本中，最大长度为 255 字节）。
   - 从 MySQL 5.0.3 版本开始，`BLOB` 和 `TEXT` 类型的列可以存储更大的数据，最大可达到 4GB（使用 `LONGBLOB` 和 `LONGTEXT`）。
3. **字符编码**：`BLOB` 存储的是二进制数据，与字符编码无关。`TEXT` 存储的是字符数据，受字符编码的影响，如 `utf8`、`latin1` 等。
4. **存储效率**：`BLOB` 由于存储的是二进制数据，不涉及字符编码转换，通常存储效率更高。`TEXT` 类型在存储时会进行字符编码转换，可能会占用更多的存储空间。
5. **排序和比较**：`BLOB` 类型的列不能进行排序和比较，因为它们是二进制数据。`TEXT` 类型的列可以进行排序和比较，因为它们是字符数据。

在选择 `BLOB` 还是 `TEXT` 时，需要根据数据的特性和应用场景来决定。如果需要存储非文本的二进制数据，应选择 `BLOB`；如果需要存储大量的文本数据，则应选择 `TEXT`。

------



## 四、索引

> - MYSQL官方对索引的定义为：索引（Index）是帮助 MySQL 高效获取数据的数据结构，所以说**索引的本质是：数据结构**
>
> - 索引的目的在于提高查询效率，可以类比字典、 火车站的车次表、图书的目录等 。
>
> - 可以简单的理解为“排好序的快速查找数据结构”，数据本身之外，<font color=#FF0000>**数据库还维护者一个满足特定查找算法的数据结构**</font>，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。下图是一种可能的索引方式示例。
>
>   ![](https://img.starfish.ink/mysql/search-index-demo.png)
>
>   左边的数据表，一共有两列七条记录，最左边的是数据记录的物理地址
>
>   为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值，和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找在一定的复杂度内获取到对应的数据，从而快速检索出符合条件的记录。
>
> - 索引本身也很大，不可能全部存储在内存中，**一般以索引文件的形式存储在磁盘上**
>
> - 平常说的索引，没有特别指明的话，就是 B+ 树（多路搜索树，不一定是二叉树）结构组织的索引。其中聚集索引，次要索引，覆盖索引，复合索引，前缀索引，唯一索引默认都是使用 B+ 树索引，统称索引。此外还有哈希索引等。
>



### 说说你对 MySQL 索引的理解？

> 这种就属于比较宽泛的问题，可以有结构条例的多说一些。差不多的问法：
>
> - 索引是越多越好吗？为什么？
> - 索引有哪些优缺点？

索引是数据库优化的重要工具，

**优势**

- 提高数据检索效率，降低数据库IO成本

- 降低数据排序的成本，降低CPU的消耗


**劣势**

- 索引也是一张表，保存了主键和索引字段，并指向实体表的记录，所以也需要占用内存
- 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行 INSERT、UPDATE 和 DELETE。
  因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，
  都会调整因为更新所带来的键值变化后的索引信息



> ##### MySQL索引分类
>
> ###### 数据结构角度
>
> - B+树索引 
> - Hash索引
> - R-Tree索引
>
> ###### 从物理存储角度
>
> - 聚集索引（clustered index） 
>
> - 非聚集索引（non-clustered index），也叫辅助索引（secondary index）
>
>   聚集索引和非聚集索引都是B+树结构
>
> ######  从逻辑角度 
>
> -  主键索引：主键索引是一种特殊的唯一索引，不允许有空值 
> -  普通索引或者单列索引：每个索引只包含单个列，一个表可以有多个单列索引
> -  多列索引（复合索引、联合索引）：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用复合索引时遵循最左前缀集合 
> -  唯一索引或者非唯一索引
> -  空间索引：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。
>    MYSQL使用SPATIAL关键字进行扩展，使得能够用于创建正规索引类型的语法创建空间索引。创建空间索引的列，必须将其声明为NOT NULL，空间索引只能在存储引擎为MYISAM的表中创建 
>



###  说一下 MySQL InnoDB 的索引原理是什么?

> 这就涉及到了好多知识点，我们可以列举几项关键点，说说**<mark>索引结构</mark>**、**<mark>聚簇索引</mark>**

**首先要明白索引（index）是在存储引擎（storage engine）层面实现的，而不是server层面**。不是所有的存储引擎都支持所有的索引类型。即使多个存储引擎支持某一索引类型，它们的实现和行为也可能有所差别。 

- **B+Tree索引**

  InnoDB 的主要索引结构是 B+ 树索引。B+ 树是一种平衡树，每个节点可以有多个子节点。与 B 树不同，B+ 树的所有数据都存储在叶子节点中，叶子节点之间通过指针相连，这使得范围查询和排序操作非常高效。

-  **聚簇索引 和 非聚簇索引**

  - InnoDB 中的主键索引就是聚簇索引。聚簇索引将数据行与索引紧密结合在一起，数据行存储在叶子节点中，因此通过主键查找数据非常高效
    - 当你创建一个表并指定主键时，InnoDB 会自动使用主键创建一个聚簇索引。
    - 如果没有显式定义主键，InnoDB 会选择一个唯一的非空索引代替。
    - 如果没有唯一非空索引，InnoDB 会自动生成一个隐藏的行 ID 作为聚簇索引

  - 辅助索引（也称为二级索引或非聚簇索引）是用于加速对非主键列的查询。辅助索引的叶子节点存储索引列的值以及对应的主键值。

    使用辅助索引进行查询时，InnoDB 首先通过辅助索引找到主键值，然后通过主键值在聚簇索引中查找实际数据。这种回表（回查）过程可能增加查询时间，但仍然比全表扫描快得多。

  

### 为什么要用 B+树？

> B-Tree VS B+Tree索引

MyISAM 和 InnoDB 存储引擎，都使用 B+Tree的数据结构，它相对与 B-Tree结构，所有的数据都存放在叶子节点上，且把叶子节点通过指针连接到一起，形成了一条数据链表，以加快相邻数据的检索效率。

B-Tree 是为磁盘等外存储设备设计的一种平衡查找树。

系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么。

InnoDB 存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位。InnoDB 存储引擎中默认每个页的大小为16KB。

而系统一个磁盘块的存储空间往往没有这么大，因此 InnoDB 每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小 16KB。InnoDB 在把磁盘数据读入到磁盘时会以页为基本单位，在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘I/O次数，提高查询效率。

B+Tree 是在 B-Tree 基础上的一种优化，使其更适合实现外存储索引结构。从 B-Tree 结构图中可以看到每个节点中不仅包含数据的 key 值，还有 data 值。而每一个页的存储空间是有限的，如果 data 数据较大时将会导致每个节点（即一个页）能存储的 key 的数量很小，当存储的数据量很大时同样会导致 B-Tree 的深度较大，增大查询时的磁盘 I/O 次数，进而影响查询效率。在 B+Tree 中，**所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上**，而非叶子节点上只存储 key 值信息，这样可以大大加大每个节点存储的 key 值数量，降低 B+Tree 的高度。![](https://img.starfish.ink/mysql/MySQL-B%2BTree-store.png)

- **节点存储内容**：
  - **B-Tree**：每个节点可以存储多个键和多个子节点的指针。键被存储在节点内部，并且可以被多次访问。
  - **B+Tree**：内部节点仅存储键作为索引，不存储实际的数据记录。所有的数据记录都存储在叶子节点中，并且叶子节点被额外的指针连接在一起，形成一个有序链表。

- **查询性能**：
  - **B-Tree**：叶子节点可能包含数据记录，也可能不包含，取决于具体的实现。在查找特定值时，可以在找到对应节点时立即返回结果。
  - **B+Tree**：所有的数据记录都存储在叶子节点中。在查找特定值时，需要访问叶子节点，但因为**叶子节点形成了有序链表**，所以范围查询和顺序访问的性能更好。
- **空间效率**：
  - **B-Tree**：因为每个节点存储更多的键和指针，所以每个节点可以有更少的子节点，树的高度可能会更小。
  - **B+Tree**：每个内部节点可以有更多子节点，因为它们只存储键的索引，这使得树更宽、更浅，减少了树的高度，提高了 I/O 效率。



### B+ 树 索引相比于其他索引类型的优势？

> 为什么MySQL 索引中用 B+tree，不用 B-tree 或者其他树，为什么不用 Hash 索引

**B+Tree 相对于 B 树 索引结构的优势：**

- B+ 树空间利用率更高：B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。

- B 树只适合随机检索，B+Tree 叶子节点采用的是双链表连接，同时支持随机检索和顺序检索

**B+Tree 相对于二叉树索引结构的优势：**

对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据（这里的查询参考上面 B+Tree 的聚簇索引的查询过程）。

而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。

**B+Tree 相对于 Hash 表存储结构的优势**：

我们知道范围查询是 MySQL 中常见的场景，但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。



> #### B+Tree 性质
>
> 在数据库中，B+ 树索引结构的高度 *h* 直接影响到进行一次索引查找所需的 I/O 次数。这是因为每次 I/O 操作通常只能读取一个磁盘块（或页）的数据。
>
> - **B+ 树高度 h**：
>
>   - B+树的高度是由树中节点的最大数量决定的，可以通过以下公式近似计算： $h≈logm(N)$
>
>   - 其中 *N* 是树中存储的总记录数，*m* 是每个磁盘块（页）可以存储的数据项数量。
>
> 当数据量 N 一定的情况下，m 越大，h 越小；而 m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的（默认 16 KB），如果数据项占的空间越小，数据项的数量越多，树的高度越低。
>
> 这就是为什么每个数据项，即索引字段要尽量的小，比如 int 占 4 字节，要比 bigint 8 字节少一半。这也是为什么 B+ 树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。
>
> > [!NOTE]
> >
> > 在实际应用中，B+树的高度和 I/O 次数会受到许多因素的影响，包括页的大小、数据的分布、索引的选择性等
>
> 当 B+ 树的数据项是复合的数据结构，比如(name,age,sex) 的时候，B+ 树是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F) 这样的数据来检索的时候，B+树会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex，最后得到检索的数据；但当 (20,F) 这样的没有 name 的数据来的时候，B+ 树就不知道下一步该查哪个节点，因为建立搜索树的时候name 就是第一个**比较因子**，必须要先根据 name 来搜索才能知道下一步去哪里查询。比如当 (张三,F) 这样的数据来检索时，B+ 树可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是 F 的数据了， 这个是非常重要的性质，就是我们说的**索引的最左匹配特性**。



### 聚集索引与非聚集索引的区别？

> MySQL 索引底层实现，叶子结点存放的是数据还是指向数据的内存地址?

聚集索引（Clustered Index）和非聚集索引（Non-clustered Index）是数据库管理系统中常见的两种索引类型，是一种**数据存储方式**的区分，特别是在 MySQL 中。

- 聚簇索引，也叫“**聚集索引**”，表示索引结构和数据一起存放的索引。

- 非聚集索引是**索引结构和数据分开存放的索引**。

因为 InnoDB  默认存储引擎的原因，我们说这个一般指的是 InnoDB 中的聚集索引和非聚集索引

**InnoDB 引擎索引结构的叶子节点的数据域，存放的就是实际的数据记录**（对于主索引，此处会存放表中所有的数据记录；对于辅助索引此处会引用主键，检索的时候通过主键到主键索引中找到对应数据行），或者说，**InnoDB 的数据文件本身就是主键索引文件**，这样的索引被称为"“**聚簇索引**”，一个表只能有一个聚簇索引。

- **InnoDB 聚集索引**：InnoDB 存储引擎使用聚集索引来存储主键列，并且所有非主键列都包含在聚集索引中，这意味着聚集索引实际上包含了整行数据。一个表只能有一个聚簇索引
- **InnoDB 非聚集索引**：InnoDB 的非聚集索引（也称为辅助索引）首先存储非主键索引列的值，然后通过主键列的值来查找对应的行。这种方式称为“索引的索引”，因为非聚集索引首先查找主键。

![](https://img.starfish.ink/mysql/MySQL-secondary-index.png)



### 非聚簇索引一定会回表查询吗?

不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。

举个简单的例子：假设我们在员工表的年龄上建立了索引，那么当进行的查询时,在索引的叶子节点上,已经包含了 age 信息，不会再次进行回表查询。

```sql
select age from employee where age < 20
```



### InnoDB引擎中的索引策略，了解过吗？

InnoDB 索引策略主要包括以下几个方面:

- 聚簇索引
- 辅助索引，也就是非聚簇索引
- 覆盖索引：查询可以直接通过索引获取所需数据，而无需回表查询
- 前缀索引：用于对较长的字符串列进行索引，只索引字符串的前 N 个字符
- 全文索引：用于对大文本字段进行全文检索

每种索引类型都有其独特的用途和优势，通过合理使用这些索引，可以显著提高数据库的查询性能。



### 使用索引查询一定能提高查询的性能吗？为什么?

> 其实这个问题也对应的是哪些情况需要建立索引，哪些不需要
>
> 使用索引需要注意的几个地方？

使用索引查询并不一定总能提高查询性能

**为什么使用索引通常能提高查询性能：**

1. 减少需要扫描的数据量来加快数据检索速度
   - 频繁作为查询条件的字段
   - 查询中与其他表关联的字段，外键关系建立索引
   - 在查询涉及的所有列都在索引中时，可以避免回表查询，提高查询效率

2. 索引可以显著加快 `ORDER BY` 和 `GROUP BY` 操作

**为什么在某些情况下索引查询反而可能降低性能**：

1. 小表或低选择性列

   - **小表**：对于行数很少的表，索引带来的性能提升有限，因为全表扫描的开销也很小。索引反而增加了额外的维护开销。

     **低选择性列**：如果索引列的选择性很低（例如，性别列只有两个值 "M" 和 "F"），使用索引可能会导致大量的行扫描，无法显著减少数据量，索引的效果不明显。

2. 经常增删改的表

   - **频繁的写操作**：索引不仅在读取数据时加速查询，还在插入、更新和删除操作时带来额外的开销。每次写操作都需要更新索引，索引越多，写操作的开销就越大。

3. 在高并发环境下，索引也可能导致锁竞争，影响查询性能

是否使用索引以及如何设计索引需要根据具体的查询模式、数据量、更新频率、硬件资源等多种因素综合考虑



### InnoDB 表为什么要建议用自增列做主键

1. **聚簇索引的效率**

   InnoDB 表使用聚集索引来存储数据，聚簇索引的叶子节点存储数据行的物理位置，这意味着数据行在磁盘上的物理顺序与索引的顺序一致

   - **顺序插入**：自增列使得每次插入新记录时，新的记录都被追加到表的末尾，插入操作非常高效，不需要在表的中间位置插入数据，从而避免了频繁的页拆分和数据移动。
   - **减少碎片**：顺序插入数据可以减少表碎片，维持数据的物理连续性，进而提高查询性能。

2. **唯一性和简单性**

   - **唯一性**：自增列保证每个记录都有一个唯一的标识符，避免了重复记录。

   - **简单性**：自增列无需复杂的生成逻辑，MySQL 会自动处理自增值，简化了应用程序的开发和维护。

3. **更好的缓存命中率**

   顺序插入的新记录使得最近插入的数据很可能在相邻的存储位置，这提高了缓存的命中率。

   - **缓存友好**：数据库缓存更容易缓存相邻的存储块，从而提高查询的性能，特别是在高并发的读写环境下。



### 如何写 SQL 能够有效的使用到复合索引

> MySQL高效索引

要有效地使用复合索引（也称为多列索引或组合索引），编写 SQL 查询时需要考虑以下几点：

1. **覆盖索引**（Covering Index），或者叫索引覆盖， 也就是平时所说的不需要回表操作 

   - 如果查询的列完全包含在复合索引中，那么可以使用覆盖索引，这样可以避免回表查询，提高性能。

2. **最左前缀法则**：

   - 复合索引的效率取决于查询条件是否遵循最左前缀法则，即从索引的最左边列开始匹配。

   - 例如，如果你有一个 (`c1`, `c2`, `c3`) 的复合索引，那么以下查询可以高效地使用索引：

     ```sql
     SELECT * FROM table WHERE c1 = 'value1';
     SELECT * FROM table WHERE c1 = 'value1' AND c2 = 'value2';
     ```

   - 如果查询条件不包含 `c1`，则该复合索引不会被使用。

3. **索引列的顺序**：

   - 在复合索引中，列的顺序很重要。应该将选择性最高的列（即不同值占总行数比例最高的列）放在前面。

4. **使用索引列作为条件**：

   - 确保 WHERE 子句中的条件列与复合索引中的列相匹配，并且顺序正确。

5. **避免使用函数和表达式**：

   - 如果在 WHERE 子句中对索引列应用了函数或计算，可能会使索引失效。
   - 例如，如果 `c1` 是索引的一部分，应避免 `WHERE UPPER(col1) = 'VALUE'`，而应使用 `WHERE c1 = 'value'`。

6. **范围查询和排序**：

   - 复合索引可以用于涉及范围查询的 ORDER BY 和 GROUP BY 子句。
   - 例如，如果有一个 (`c1`, `c2`) 的索引，`ORDER BY c1, c2` 可以有效地使用索引。

7. **限制索引的使用**：

   - 使用 `LIKE` 操作符进行模糊匹配时，如果模式以通配符（`%`）开头，索引将不会被使用。
   - 例如，使用 `WHERE c1 LIKE '%value'` 将无法利用索引。

考虑查询的实际条件，如数据量大小、表的更新频率等，以确定是否真正需要复合索引。在实际的数据库环境中测试查询性能，并根据查询执行计划（`EXPLAIN`）来优化索引的使用



### 主键索引和唯一索引的区别？

主键索引是特殊的唯一索引，唯一索引查询会涉及到“回表”操作

| 特性          | 主键索引（Primary Key Index） | 唯一索引（Unique Index）                                     |
| ------------- | ----------------------------- | ------------------------------------------------------------ |
| 唯一性        | 必须唯一                      | 必须唯一（允许 NULL 值）                                     |
| 是否允许 NULL | 不允许                        | 允许多个 NULL 值「这里 NULL 的定义 ，是指 未知值。 所以多个 NULL ，都是未知的」 |
| 聚簇索引      | 是（在 InnoDB 中）            | 否（除非是主键）                                             |
| 每个表的数量  | 只能有一个                    | 可以有多个                                                   |
| 主要用途      | 唯一标识每一行                | 强制唯一性约束，非主键用途                                   |
| 创建语法      | `PRIMARY KEY`                 | `UNIQUE`                                                     |

```sql
CREATE TABLE example (
    id INT AUTO_INCREMENT PRIMARY KEY,
    email VARCHAR(100) UNIQUE
);
INSERT INTO example (email) VALUES (NULL), (NULL), (NULL);
```

------



## 五、日志

> Mysql 日志其实是各种其他知识模块的基础。
>
> MySQL日志文件：用来记录 MySQL 实例对某种条件做出响应时写入的文件，大概可分为：通用查询日志、慢查询日志、错误日志、二进制日志、中继日志、重做日志和回滚日志。

### 说下 一条 MySQL 更新语句的执行流程是怎样的吧？

1. 先在 B+ 树中定位到该记录（这个过程也被称作**加锁读**），如果该记录所在的页面不在 buffer pool 里，先将其加载到 buffer pool 里再读取。

2. 首先更新聚簇索引记录。 更新聚簇索引记录时： 

   ① 先向 Undo 页面写 undo 日志。不过由于这是在更改页面，所以修改 Undo 页面前需要先记录一下相应的 redo 日志。 

   ② 将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。

   > 这里可以会有点疑惑。我们可以直接理解成先写 undo 再写 redo，这里修改后的页面并没有加入 buffer pool 的 flush 链表，记录的 redo 日志也没有加入到 redo log buffer。当这个函数执行完后，才会：先将这个过程产生的 redo 日志写入到 redo log buffer，再将这个过程修改的页面加入到 buffer pool 的 flush 链表中。

3. 更新其他的二级索引记录。

   > 更新二级索引记录时不会再记录 undo 日志，但由于是在修改页面内容，会先记录相应的 redo 日志。

4. 记录该语句对应的 binlog 日志，此时记录的 binlog 并没有刷新到硬盘上的 binlog 日志文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 日志刷新到硬盘。

5. 引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。



### MySQL  都有哪些日志，执行顺序是怎么样的？

时序上先 undo log，redo log 先 prepare， 再写 binlog，最后再把 redo log commit

![](https://img.starfish.ink/mysql/log-seq.png)



### 说说 redo log 、undo log 和 bin log ？

在 MySQL 中，特别是在使用 InnoDB 存储引擎时，`redo log`（重做日志）、`undo log`（回滚日志）和 `binlog`（二进制日志）各自承担着不同的角色：

**1. Redo Log（重做日志）：**

- **目的**：确保事务的持久性。在系统崩溃后，`redo log` 允许恢复未提交的事务更改，保证数据的完整性和一致性。
- **内容**：记录了事务对数据页所做的物理更改【**物理日志**】，以便在崩溃恢复时重新应用这些更改。
- **写入时机**：在事务提交时，将更改刷新到磁盘上的 `redo log` 文件中。
- **大小和循环**：`redo log` 通常配置为固定大小的日志文件，并且可以循环使用。
- **用途**：**崩溃恢复**，在数据库崩溃后，通过 redo log 恢复到崩溃前的状态，保证数据一致性。

**2. Undo Log（回滚日志）：**

- **目的**：提供事务的原子性和一致性。它允许撤销事务的更改，以保持数据的一致状态。
- **内容**：记录了事务对数据页所做的更改的逆操作【**逻辑日志**】，使得在事务失败或需要回滚时可以恢复原始数据。
- **写入时机**：当事务进行修改操作时，`undo log` 会记录这些更改的逆操作，通常在事务提交前就已经写入。
- **用途**：主要用于 MVCC（多版本并发控制）和事务回滚。

**3. Binlog（二进制日志）：**

- **目的**：记录数据库的所有修改操作，用于数据恢复、主从复制和数据审计。
- **内容**：记录了所有修改数据的 SQL 语句，如 `INSERT`、`UPDATE` 和 `DELETE`，但不记录 `SELECT` 和 `SHOW` 这类的语句。
- **写入时机**：在 SQL 语句执行后，根据配置，`binlog` 可以立即或事务提交时写入磁盘。
- **大小和存储**：`binlog` 文件通常不循环使用，它们会随着时间持续增长，直到通过配置的策略进行清理。

| 特性     | Redo Log               | Undo Log                         | Bin Log                |
| -------- | ---------------------- | -------------------------------- | ---------------------- |
| 主要用途 | 崩溃恢复               | 事务回滚、多版本并发控制（MVCC） | 主从复制和数据恢复     |
| 日志类型 | 物理日志               | 逻辑日志                         | 逻辑日志               |
| 存储内容 | 页级物理更改           | 数据快照                         | SQL 语句或行级数据变化 |
| 写入方式 | 循环写入               | 按需写入                         | 追加写入               |
| 写入时机 | 事务提交时             | 事务操作时                       | 事务提交时             |
| 大小     | 固定大小               | 可变大小                         | 可变大小               |
| 作用     | 保证数据一致性和持久性 | 提供事务回滚和一致性读支持       | 实现数据复制和备份     |



### MySQL 的 binlog 有几种录入格式？分别有什么区别？

`binlog`日志有三种格式，分别为`STATMENT`、`ROW`和`MIXED`。

> 在 `MySQL 5.7.7`之前，默认的格式是`STATEMENT`，`MySQL 5.7.7`之后，默认值是 `ROW`。日志格式通过 `binlog-format` 指定。

- `STATMENT` ：基于 SQL 语句的复制(`statement-based replication, SBR`)，每一条会修改数据的 sql 语句会记录到 binlog 中**。**
  - 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO, 从而提高了性能； 
  - 缺点：在某些情况下会导致主从数据不一致，比如执行`sysdate()`、`slepp()`等。

- `ROW` ：基于行的复制(`row-based replication, RBR`)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了**。 **
  - 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题**；**
  - 缺点：会产生大量的日志，尤其是 `alter table` 的时候会让日志暴涨

- `MIXED` ：基于 STATMENT 和 ROW 两种模式的混合复制(`mixed-based replication, MBR`)，mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式



### 为什么需要两阶段提交

以 `update T set c=c+1 where ID=2;` 为例，反向说明下原因

1. **先写 redo log 后写 binlog**。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。
   但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。
   然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
2. **先写 binlog 后写 redo log**。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

------



## 六、MySQL 事务

> 事务的隔离级别有哪些？MySQL的默认隔离级别是什么？
>
> 什么是幻读，脏读，不可重复读呢？
>
> MySQL事务的四大特性以及实现原理
>
> MVCC熟悉吗，它的底层原理？

MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ 



### ACID — 事务基本要素

![](https://tva1.sinaimg.cn/large/007S8ZIlly1geu10kkswnj305q05mweo.jpg)

事务是由一组SQL语句组成的逻辑处理单元，具有4个属性，通常简称为事务的ACID属性。

- **A (Atomicity) 原子性**：整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样
- **C (Consistency) 一致性**：在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏
- **I (Isolation)隔离性**：一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰
- **D (Durability) 持久性**：在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚



**并发事务处理带来的问题**

- 更新丢失（Lost Update)： 事务A和事务B选择同一行，然后基于最初选定的值更新该行时，由于两个事务都不知道彼此的存在，就会发生丢失更新问题 
- 脏读(Dirty Reads)：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据
- 不可重复读（Non-Repeatable Reads)：事务 A 多次读取同一数据，事务B在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。
- 幻读（Phantom Reads)：幻读与不可重复读类似。它发生在一个事务A读取了几行数据，接着另一个并发事务B插入了一些数据时。在随后的查询中，事务A就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。



**幻读和不可重复读的区别：**

- **不可重复读的重点是修改**：在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样。（因为中间有其他事务提交了修改）
- **幻读的重点在于新增或者删除**：在同一事务中，同样的条件,，第一次和第二次读出来的记录数不一样。（因为中间有其他事务提交了插入/删除）



**并发事务处理带来的问题的解决办法：**

- “更新丢失”通常是应该完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。

- “脏读” 、 “不可重复读”和“幻读” ，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决：

  - 一种是加锁：在读取数据前，对其加锁，阻止其他事务对数据进行修改。
  - 另一种是数据多版本并发控制（MultiVersion Concurrency Control，简称 **MVCC** 或 MCC），也称为多版本数据库：不用加任何锁， 通过一定机制生成一个数据请求时间点的一致性数据快照 （Snapshot)， 并用这个快照来提供一定级别 （语句级或事务级） 的一致性读取。从用户的角度来看，好象是数据库可以提供同一数据的多个版本。



### 事务隔离级别

数据库事务的隔离级别有4种，由低到高分别为

- **READ-UNCOMMITTED(读未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。
- **READ-COMMITTED(读已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。
- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。
- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。

查看当前数据库的事务隔离级别：  

```mysql
show variables like 'tx_isolation'
```

下面通过事例一一阐述在事务的并发操作中可能会出现脏读，不可重复读，幻读和事务隔离级别的联系。

数据库的事务隔离越严格，并发副作用越小，但付出的代价就越大，因为事务隔离实质上就是使事务在一定程度上“串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏感，可能更关心数据并发访问的能力。

#### Read uncommitted

读未提交，就是一个事务可以读取另一个未提交事务的数据。

事例：老板要给程序员发工资，程序员的工资是3.6万/月。但是发工资时老板不小心按错了数字，按成3.9万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。

分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是脏读。

那怎么解决脏读呢？Read committed！读提交，能解决脏读问题。

#### Read committed

读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。

事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的…

分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是**不可重复读**。

那怎么解决可能的不可重复读问题？Repeatable read ！

#### Repeatable read

重复读，就是在开始读取数据（事务开启）时，不再允许修改操作。 <mark>**MySQL的默认事务隔离级别** </mark>

事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。

分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，**不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作**。

**什么时候会出现幻读？**

事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增INSERT了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。

那怎么解决幻读问题？Serializable！

#### Serializable 序列化

Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。简单来说，Serializable会在读取的每一行数据上都加锁，所以可能导致大量的超时和锁争用问题。这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。



> **简单点的理解**
>
> 读未提交：别人改数据的事务尚未提交，我在我的事务中也能读到。 
>
> 读已提交：别人改数据的事务已经提交，我在我的事务中才能读到。 
>
> 可重复读：别人改数据的事务已经提交，我在我的事务中也不去读。 
>
> 串行：我的事务尚未提交，别人就别想改数据。



#### 比较

| 事务隔离级别                 | 读数据一致性                             | 脏读 | 不可重复读 | 幻读 |
| ---------------------------- | ---------------------------------------- | ---- | ---------- | ---- |
| 读未提交（read-uncommitted） | 最低级被，只能保证不读取物理上损坏的数据 | 是   | 是         | 是   |
| 读已提交（read-committed）   | 语句级                                   | 否   | 是         | 是   |
| 可重复读（repeatable-read）  | 事务级                                   | 否   | 否         | 是   |
| 串行化（serializable）       | 最高级别，事务级                         | 否   | 否         | 否   |

需要说明的是，事务隔离级别和数据访问的并发性是对立的，事务隔离级别越高并发性就越差。所以要根据具体的应用来确定合适的事务隔离级别，这个地方没有万能的原则。



### 什么是幻读？如何解决的

> “幻读”面试关注点：
>
> 1. 要想解决幻读不能升级事务隔离级别到“可串行化”，那样数据库也失去了并发处理能力。
>
> 2. 行锁解决不了幻读，因为即使锁住所有记录，还是阻止不了插入新数据。
>
> 3. 解决幻读的办法是锁住记录之间的“间隙”，为此 MySQL InnoDB 引入了新的锁，叫间隙锁（Gap Lock），所以在面试中，你也要掌握间隙锁，以及间隙锁与行锁结合的 next-key lock 锁。

1. 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。
2. 幻读仅专指“新插入的行”

> 由于在 RR 级别下，普通的读是快照读（一致性读），所以<mark>幻读仅发生在当前读的基础上</mark>。
>
> > 一致性读也被称为**快照读**，当我们查询数据库在某个时间点的快照时，只能看到这个时间点之前事务提交更新的结果，而不能看到这个时间点之后事务提交的更新结果。
> >
> > 与 **快照读** 相对应的则是 **当前读**，**当前读**就是读取最新数据，而不是历史版本的数据。
>
> 举例来说：
>
> `select * from t where d=0` 就是快照读，对于同一个事务来说，每次读到的结果是一样的。
>
> `select * from t where d=0 in share mode` 或 `select * from t where d=0 for update` 就是当前读，总是读取当前数据行的最新版本
>
> 所谓当前读，指的是加锁的 select(S或者X), update, delete 等语句。在 RR 的事务隔离级别下，数据库会使用next-key locks来锁住本条记录以及索引区间。
>
> 拿上面那个例子来说，在 RR 的情况下，假设使用的是当前读，加锁了的读
>
> select * from table where id>3 锁住的就是 id=3 这条记录以及 id>3 这个区间范围，锁住索引记录之间的范围，避免范围间插入记录，以避免产生幻影行记录。

MySQL InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）**。我们可以通过`SELECT @@tx_isolation;`命令来查看，MySQL 8.0 该命令改为`SELECT @@transaction_isolation;`

这里需要注意的是：与 SQL 标准不同的地方在于InnoDB 存储引擎在 **REPEATABLE-READ（可重读）**事务隔离级别下使用的是 Next-Key Lock 算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说 InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）已经可以完全保证事务的隔离性要求，即达到了 SQL标准的 **SERIALIZABLE(可串行化)**隔离级别，而且保留了比较好的并发性能。

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是**READ-COMMITTED(读已提交):**，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）**并不会有任何性能损失。



> 加锁的基本原则（RR隔离级别下）
> 原则1：加锁的对象是next-key lock。（是一个前开后闭的区间）
> 原则2：查找过程中访问到的对象才加锁
> 优化1：唯一索引加锁时，next-key lock退化为行锁。
> 索引上的等值查询，向右遍历时最后一个不满足等值条件的时候，next-key lock 退化为间隙锁
> 唯一索引和普通索引在范围查询的时候 都会访问到不满足条件的第一个值为止

### MVCC 多版本并发控制

MySQL的大多数事务型存储引擎实现都不是简单的行级锁。基于提升并发性考虑，一般都同时实现了多版本并发控制（MVCC），包括Oracle、PostgreSQL。只是实现机制各不相同。

可以认为 MVCC 是行级锁的一个变种，但它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只是锁定必要的行。

MVCC 的实现是通过保存数据在某个时间点的快照来实现的。也就是说不管需要执行多长时间，每个事物看到的数据都是一致的。

典型的 MVCC 实现方式，分为**乐观（optimistic）并发控制和悲观（pressimistic）并发控制**。下边通过 InnoDB 的简化版行为来说明 MVCC 是如何工作的。

InnoDB 的 MVCC，是通过在每行记录后面保存两个隐藏的列来实现。这两个列，一个保存了行的创建时间，一个保存行的过期时间（删除时间）。当然存储的并不是真实的时间，而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。 

**REPEATABLE READ（可重读）隔离级别下 MVCC 如何工作：**

- SELECT

  InnoDB 会根据以下两个条件检查每行记录：

  - InnoDB 只查找版本早于当前事务版本的数据行，这样可以确保事务读取的行，要么是在开始事务之前已经存在要么是事务自身插入或者修改过的

  - 行的删除版本号要么未定义，要么大于当前事务版本号，这样可以确保事务读取到的行在事务开始之前未被删除

  只有符合上述两个条件的才会被查询出来

- INSERT：InnoDB 为新插入的每一行保存当前系统版本号作为行版本号

- DELETE：InnoDB 为删除的每一行保存当前系统版本号作为行删除标识

- UPDATE：InnoDB 为插入的一行新纪录保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为删除标识


保存这两个额外系统版本号，使大多数操作都不用加锁。使数据操作简单，性能很好，并且也能保证只会读取到符合要求的行。不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作和一些额外的维护工作。

**MVCC 只在 COMMITTED READ（读提交）和REPEATABLE READ（可重复读）两种隔离级别下工作**。



#### MVCC 解决了什么问题

- 读写不冲突，极大的增加了系统的并发性能
- 解决脏读，幻读，不可重复读 等问题（注:其实多版本只是解决不可重复读问题，而加上间隙锁(也 就是它这里所谓的并发控制)才解决了幻读问题。）

> InnoDB下的Compact行结构，有三个隐藏的列
>
> | 列名           | 是否必须 | 描述                                                         |
> | -------------- | -------- | ------------------------------------------------------------ |
> | row_id         | 否       | 行ID，唯一标识一条记录（如果定义主键，它就没有啦）           |
> | transaction_id | 是       | 事务ID                                                       |
> | roll_pointer   | 是       | DB_ROLL_PTR是一个回滚指针，用于配合undo日志，指向上一个旧版本 |



### 事务日志

InnoDB 使用日志来减少提交事务时的开销。因为日志中已经记录了事务，就无须在每个事务提交时把缓冲池的脏块刷新(flush)到磁盘中。

事务修改的数据和索引通常会映射到表空间的随机位置，所以刷新这些变更到磁盘需要很多随机 IO。

InnoDB 假设使用常规磁盘，随机IO比顺序IO昂贵得多，因为一个IO请求需要时间把磁头移到正确的位置，然后等待磁盘上读出需要的部分，再转到开始位置。

InnoDB 用日志把随机 IO 变成顺序 IO。一旦日志安全写到磁盘，事务就持久化了，即使断电了，InnoDB可以重放日志并且恢复已经提交的事务。

InnoDB 使用一个后台线程智能地刷新这些变更到数据文件。这个线程可以批量组合写入，使得数据写入更顺序，以提高效率。

事务日志可以帮助提高事务效率：

- 使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘。
- 事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头，所以采用事务日志的方式相对来说要快得多。
- 事务日志持久以后，内存中被修改的数据在后台可以慢慢刷回到磁盘。
- 如果数据的修改已经记录到事务日志并持久化，但数据本身没有写回到磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这一部分修改的数据。

目前来说，大多数存储引擎都是这样实现的，我们通常称之为**预写式日志**（Write-Ahead Logging），修改数据需要写两次磁盘。



### 事务的实现

事务的实现是基于数据库的存储引擎。不同的存储引擎对事务的支持程度不一样。MySQL 中支持事务的存储引擎有 InnoDB 和 NDB。 

事务的实现就是如何实现 ACID 特性。

事务的隔离性是通过锁实现，而事务的原子性、一致性和持久性则是通过事务日志实现 。



> 事务是如何通过日志来实现的，说得越深入越好。

事务日志包括：**重做日志redo**和**回滚日志undo**

- **redo log（重做日志**） 实现持久性

  在innoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志先行”(Write-Ahead Logging)。当事务提交之后，在Buffer Pool中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态。未完成的事务，可以继续提交，也可以选择回滚，这基于恢复的策略而定。

  在系统启动的时候，就已经为redo log分配了一块连续的存储空间，以顺序追加的方式记录Redo Log，通过顺序IO来改善性能。所有的事务共享redo log的存储空间，它们的Redo Log按语句的执行顺序，依次交替的记录在一起。

- **undo log（回滚日志）**  实现原子性

  undo log 主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录了数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。单个事务的回滚，只会回滚当前事务做的操作，并不会影响到其他的事务做的操作。 

  Undo记录的是已部分完成并且写入硬盘的未完成的事务，默认情况下回滚日志是记录下表空间中的（共享表空间或者独享表空间）
  
  > 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。
  >
  > ![](https://static001.geekbang.org/resource/image/d9/ee/d9c313809e5ac148fc39feff532f0fee.png)
  >
  > 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。

二种日志均可以视为一种恢复操作，redo_log是恢复提交事务修改的页操作，而undo_log是回滚行记录到特定版本。二者记录的内容也不同，redo_log是物理日志，记录页的物理修改操作，而undo_log是逻辑日志，根据每行记录进行记录。



> 又引出个问题：你知道MySQL 有多少种日志吗？

- **错误日志**：记录出错信息，也记录一些警告信息或者正确的信息。

- **查询日志**：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。

- **慢查询日志**：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。

- **二进制日志**：记录对数据库执行更改的所有操作。

- **中继日志**：中继日志也是二进制日志，用来给slave 库恢复

- **事务日志**：重做日志redo和回滚日志undo



> 分布式事务相关问题，可能还会问到 2PC、3PC，，，

### MySQL对分布式事务的支持

分布式事务的实现方式有很多，既可以采用 InnoDB 提供的原生的事务支持，也可以采用消息队列来实现分布式事务的最终一致性。这里我们主要聊一下 InnoDB 对分布式事务的支持。

MySQL 从 5.0.3  InnoDB 存储引擎开始支持 XA 协议的分布式事务。一个分布式事务会涉及多个行动，这些行动本身是事务性的。所有行动都必须一起成功完成，或者一起被回滚。

在MySQL中，使用分布式事务涉及一个或多个资源管理器和一个事务管理器。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gj0sqpwsvdj30k009dtak.jpg)

如图，MySQL 的分布式事务模型。模型中分三块：应用程序（AP）、资源管理器（RM）、事务管理器（TM）:

- 应用程序：定义了事务的边界，指定需要做哪些事务；
- 资源管理器：提供了访问事务的方法，通常一个数据库就是一个资源管理器；
- 事务管理器：协调参与了全局事务中的各个事务。

分布式事务采用两段式提交（two-phase commit）的方式：

- 第一阶段所有的事务节点开始准备，告诉事务管理器ready。
- 第二阶段事务管理器告诉每个节点是commit还是rollback。如果有一个节点失败，就需要全局的节点全部rollback，以此保障事务的原子性。



### 事务的 ACID 分别是如何实现的

从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。也就是说ACID四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。数据库必须要实现AID三大特性，才有可能实现一致性。例如，原子性无法保证，显然一致性也无法保证。



原子性保证：`undo log`名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的sql语句，他需要记录你要回滚的相应日志信息。

持久性保证：`redo log` mysql修改数据同时在内存和redo log记录这次操作，宕机的时候可以从redo log恢复。

> Mysql是先把磁盘上的数据加载到内存中，在内存中对数据进行修改，再刷回磁盘上。如果此时突然宕机，内存中的数据就会丢失。
>
> *怎么解决这个问题？*
>
> 简单啊，事务提交前直接把数据写入磁盘就行啊。
>
> *这么做有什么问题？*
>
> - 只修改一个页面里的一个字节，就要将整个页面刷入磁盘，太浪费资源了。毕竟一个页面16kb大小，你只改其中一点点东西，就要将16kb的内容刷入磁盘，听着也不合理。
> - 毕竟一个事务里的SQL可能牵涉到多个数据页的修改，而这些数据页可能不是相邻的，也就是属于随机IO。显然操作随机IO，速度会比较慢。
>
> 于是，决定采用`redo log`解决上面的问题。当做数据修改的时候，不仅在内存中操作，还会在`redo log`中记录这次操作。当事务提交的时候，会将`redo log`日志进行刷盘(`redo log`一部分在内存中，一部分在磁盘上)。当数据库宕机重启的时候，会将`redo log`中的内容恢复到数据库中，再根据`undo log`和`binlog`内容决定回滚数据还是提交数据。
>
> *采用redo log的好处？*
>
> 其实好处就是将`redo log`进行刷盘比对数据页刷盘效率高，具体表现如下
>
> - `redo log`体积小，毕竟只记录了哪一页修改了啥，因此体积小，刷盘快。
> - `redo log`是一直往末尾进行追加，属于顺序IO。效率显然比随机IO来的快。

隔离性保证：利用的是锁和MVCC机制

一致性保证：一致性由其他三大特性保证、程序代码要保证业务上的一致性



------



## 六、MySQL查询

> count(*) 和 count(1)和count(列名)区别   ps：这道题说法有点多

执行效果上：

- count(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为NULL 
- count(1)包括了所有列，用1代表代码行，在统计结果的时候，不会忽略列值为NULL 
- count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是指空字符串或者0，而是表示null）的计数，即某个字段值为NULL时，不统计。

执行效率上：

- 列名为主键，count(列名)会比count(1)快 
- 列名不为主键，count(1)会比count(列名)快
- 如果表多个列并且没有主键，则 count(1) 的执行效率优于 count(*)
- 如果有主键，则 select count（主键）的执行效率是最优的 
- 如果表只有一个字段，则 select count(*) 最优。



### MySQL中 in和 exists 的区别？

- exists：exists对外表用loop逐条查询，每次查询都会查看exists的条件语句，当exists里的条件语句能够返回记录行时（无论记录行是的多少，只要能返回），条件就为真，返回当前loop到的这条记录；反之，如果exists里的条件语句不能返回记录行，则当前loop到的这条记录被丢弃，exists的条件就像一个bool条件，当能返回结果集则为true，不能返回结果集则为false
- in：in查询相当于多个or条件的叠加

```mysql
SELECT * FROM A WHERE A.id IN (SELECT id FROM B);
SELECT * FROM A WHERE EXISTS (SELECT * from B WHERE B.id = A.id);
```

**如果查询的两个表大小相当，那么用in和exists差别不大**。 

如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in：



> UNION和UNION ALL的区别?

UNION和UNION ALL都是将两个结果集合并为一个，**两个要联合的SQL语句 字段个数必须一样，而且字段类型要“相容”（一致）；**

- UNION在进行表连接后会筛选掉重复的数据记录（效率较低），而UNION ALL则不会去掉重复的数据记录；

- UNION会按照字段的顺序进行排序，而UNION ALL只是简单的将两个结果合并就返回；



### SQL执行顺序

- 手写

  ```mysql
  SELECT DISTINCT <select_list>
  FROM  <left_table> <join_type>
  JOIN  <right_table> ON <join_condition>
  WHERE  <where_condition>
  GROUP BY  <group_by_list>
  HAVING <having_condition>
  ORDER BY <order_by_condition>
  LIMIT <limit_number>
  ```

- 机读

  ```mysql
  FROM  <left_table>
  ON <join_condition>
  <join_type> JOIN  <right_table> 
  WHERE  <where_condition>
  GROUP BY  <group_by_list>
  HAVING <having_condition>
  SELECT
  DISTINCT <select_list>
  ORDER BY <order_by_condition>
  LIMIT <limit_number>
  ```

- 总结

  ![sql-parse](https://tva1.sinaimg.cn/large/007S8ZIlly1gf3t8jyy81j30s2083wg2.jpg)

  

> mysql 的内连接、左连接、右连接有什么区别？
>
> 什么是内连接、外连接、交叉连接、笛卡尔积呢？

### Join图

![sql-joins](https://tva1.sinaimg.cn/large/007S8ZIlly1gf3t8novxpj30qu0l4wi7.jpg)

------



## 八、MySQL 锁机制

> 数据库的乐观锁和悲观锁？
>
> MySQL 中有哪几种锁，列举一下？
>
> MySQL中InnoDB引擎的行锁是怎么实现的？
>
> MySQL 间隙锁有没有了解，死锁有没有了解，写一段会造成死锁的 sql 语句，死锁发生了如何解决，MySQL 有没有提供什么机制去解决死锁

锁是计算机协调多个进程或线程并发访问某一资源的机制。

在数据库中，除传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供许多用户共享的资源。数据库锁定机制简单来说，就是数据库为了保证数据的一致性，而使各种共享资源在被并发访问变得有序所设计的一种规则。

打个比方，我们到淘宝上买一件商品，商品只有一件库存，这个时候如果还有另一个人买，那么如何解决是你买到还是另一个人买到的问题？这里肯定要用到事务，我们先从库存表中取出物品数量，然后插入订单，付款后插入付款表信息，然后更新商品数量。在这个过程中，使用锁可以对有限的资源进行保护，解决隔离和并发的矛盾。

 

### 锁的分类

**从对数据操作的类型分类**：

- **读锁**（共享锁）：针对同一份数据，多个读操作可以同时进行，不会互相影响

- **写锁**（排他锁）：当前写操作没有完成前，它会阻断其他写锁和读锁

**从对数据操作的粒度分类**：


为了尽可能提高数据库的并发度，每次锁定的数据范围越小越好，理论上每次只锁定当前操作的数据的方案会得到最大的并发度，但是管理锁是很耗资源的事情（涉及获取，检查，释放锁等动作），因此数据库系统需要在高并发响应和系统性能两方面进行平衡，这样就产生了“锁粒度（Lock granularity）”的概念。

- **表级锁**：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低（MyISAM 和 MEMORY 存储引擎采用的是表级锁）；

- **行级锁**：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高（InnoDB 存储引擎既支持行级锁也支持表级锁，但默认情况下是采用行级锁）；  

- **页面锁**：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。

适用：从锁的角度来说，表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。

|        | 行锁 | 表锁 | 页锁 |
| ------ | ---- | ---- | ---- |
| MyISAM |      | √    |      |
| BDB    |      | √    | √    |
| InnoDB | √    | √    |      |
| Memory |      | √    |      |



### MyISAM 表锁

MyISAM 的表锁有两种模式：

- 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；
- 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作；

MyISAM 表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后， 只有持有锁的线程可以对表进行更新操作。 其他线程的读、 写操作都会等待，直到锁被释放为止。

默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求。



### InnoDB 行锁

InnoDB 实现了以下两种类型的**行锁**：

- 共享锁又称为读锁，简称  S 锁，顾名思义，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。

- 排他锁又称为写锁，简称 X 锁，顾名思义，排他锁就是不能与其他所并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据行读取和修改。

为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是**表锁**：

- 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。
- 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。

**索引失效会导致行锁变表锁**。比如 varchar 查询不写单引号的情况。

**InnoDB 的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为表锁**。



#### 加锁机制

**乐观锁与悲观锁是两种并发控制的思想，可用于解决丢失更新问题** 

乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务。用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式

悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁。另外与乐观锁相对应的，**悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。**



#### 锁模式(InnoDB有三种行锁的算法)

- **记录锁(Record Locks)**： 单个行记录上的锁。对索引项加锁，锁定符合条件的行。其他事务不能修改和删除加锁项； 

  ```mysql
  SELECT * FROM table WHERE id = 1 FOR UPDATE;
  ```

  它会在 id=1 的记录上加上记录锁，以阻止其他事务插入，更新，删除 id=1 这一行

  在通过 主键索引 与 唯一索引 对数据行进行 UPDATE 操作时，也会对该行数据加记录锁：

  ```mysql
  -- id 列为主键列或唯一索引列
  UPDATE SET age = 50 WHERE id = 1;
  ```

- **间隙锁（Gap Locks）**： 当我们使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB 会给符合条件的已有数据记录的索引项加锁。对于键值在条件范围内但并不存在的记录，叫做“间隙”。

  InnoDB 也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁。

  > 如下表，初始化插入了 6 个记录，这就产生了 7 个间隙。
  >
  > ![](https://static001.geekbang.org/resource/image/e7/61/e7f7ca0d3dab2f48c588d714ee3ac861.png)
  >
  > 这样，当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的 6 个记录加上了行锁，还同时加了 7 个间隙锁。这样就确保了无法再插入新的记录。

  对索引项之间的“间隙”加锁，锁定记录的范围（对第一条记录前的间隙或最后一条将记录后的间隙加锁），不包含索引项本身。其他事务不能在锁范围内插入数据，这样就防止了别的事务新增幻影行。 

  间隙锁基于非唯一索引，它锁定一段范围内的索引记录。间隙锁基于下面将会提到的`Next-Key Locking` 算法，请务必牢记：**使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据**。

  ```mysql
  SELECT * FROM table WHERE id BETWEN 1 AND 10 FOR UPDATE;
  ```

  即所有在`（1，10）`区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。

  GAP 锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况

- **临键锁(Next-key Locks)**： **临键锁**，是**记录锁与间隙锁的组合**，它的封锁范围，既包含索引记录，又包含索引区间。(临键锁的主要目的，也是为了避免**幻读**(Phantom Read)。如果把事务的隔离级别降级为RC，临键锁则也会失效。)

  Next-Key 可以理解为一种特殊的**间隙锁**，也可以理解为一种特殊的**算法**。通过**临建锁**可以解决幻读的问题。 每个数据行上的非唯一索引列上都会存在一把临键锁，当某个事务持有该数据行的临键锁时，会锁住一段左开右闭区间的数据。需要强调的一点是，<mark>`InnoDB` 中行级锁是基于索引实现的，临键锁只与非唯一索引列有关，在唯一索引列（包括主键列）上不存在临键锁</mark>。

  对于行的查询，都是采用该方法，主要目的是解决幻读的问题。

> select for update有什么含义，会锁表还是锁行还是其他

for update 仅适用于 InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。在进行事务操作时，通过“for update”语句，MySQL  会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁。

InnoDB 这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB 才使用行级锁，否则，InnoDB 将使用表锁！

假设有个表单 products ，里面有 id 跟 name 二个栏位，id 是主键。

- 明确指定主键，并且有此笔资料，row lock

```mysql
SELECT * FROM products WHERE id='3' FOR UPDATE;
SELECT * FROM products WHERE id='3' and type=1 FOR UPDATE;
```

- 明确指定主键，若查无此笔资料，无lock

```mysql
SELECT * FROM products WHERE id='-1' FOR UPDATE;
```

- 无主键，table lock

```mysql
SELECT * FROM products WHERE name='Mouse' FOR UPDATE;
```

- 主键不明确，table lock

```mysql
SELECT * FROM products WHERE id <>'3' FOR UPDATE;
```

- 主键不明确，table lock

```mysql
SELECT * FROM products WHERE id LIKE '3' FOR UPDATE;
```

**注1**: FOR UPDATE 仅适用于 InnoDB，且必须在交易区块(BEGIN/COMMIT)中才能生效。
**注2**: 要测试锁定的状况，可以利用 MySQL 的 Command Mode ，开二个视窗来做测试。



> MySQL 遇到过死锁问题吗，你是如何解决的？

### 死锁

**死锁产生**：

- 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环
- 当事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁
- 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会——死锁有双重原因：真正的数据冲突；存储引擎的实现方式。

> 死锁产生的四个必要条件
>
> 1. 互斥： 多个线程不能同时使用一个资源。比如线程 A 已经持有的资源，不能再同时被线程 B 持有。
> 2. 持有并等待： 当线程 A 已经持有了资源 1，又提出申请资源 2，但是资源 2 已经被线程 C 占用，所以线程 A 就会处于等待状态，但它在等待资源 2 的同时并不会释放自己已经获取的资源 1。
> 3. 不可剥夺： 线程 A 获取到资源 1 之后，在自己使用完之前不能被其他线程（比如线程 B）抢占使用。如果线程 B 也想使用资源 1，只能在线程 A 使用完后，主动释放后再获取
> 4. 循环等待： 发生死锁时，必然会存在一个线程，也就是资源的环形链。比如线程 A 已经获取了资源 1，但同时又请求获取资源 2。线程 B 已经获取了资源 2，但同时又请求获取资源 1，这就会形成一个线程和资源请求等待的环形图。

**检测死锁**：数据库系统实现了各种死锁检测和死锁超时的机制。InnoDB 存储引擎能检测到死锁的循环依赖并立即返回一个错误。

**死锁恢复**：死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁，**InnoDB 目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚**。所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可。

**外部锁的死锁检测**：发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁， 这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决

**死锁影响性能**：死锁会影响性能而不是会产生严重错误，因为InnoDB会自动检测死锁状况并回滚其中一个受影响的事务。在高并发系统上，当许多线程等待同一个锁时，死锁检测可能导致速度变慢。 有时当发生死锁时，禁用死锁检测（使用innodb_deadlock_detect配置选项）可能会更有效，这时可以依赖`innodb_lock_wait_timeout`设置进行事务回滚。



**MyISAM 避免死锁**：

- 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。

**InnoDB 避免死锁**：

- 为了在单个 InnoDB 表上执行多个并发写入操作时避免死锁，可以在事务开始时通过为预期要修改的每个元祖（行）使用`SELECT ... FOR UPDATE`语句来获取必要的锁，即使这些行的更改语句是在之后才执行的。
- 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁、更新时再申请排他锁，因为这时候当用户再申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁
- 如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以大大降低产生死锁的机会
- 通过`SELECT ... LOCK IN SHARE MODE` 获取行的读锁后，如果当前事务再需要对该记录进行更新操作，则很有可能造成死锁。
- 改变事务隔离级别

如果出现死锁，可以用 `show engine innodb status; `命令来确定最后一个死锁产生的原因。返回结果中包括死锁相关事务的详细信息，如引发死锁的 SQL 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。据此可以分析死锁产生的原因和改进措施。



### 如何尽可能避免死锁

1. 合理的设计索引，区分度高的列放到组合索引前面，使业务 SQL 尽可能通过索引`定位更少的行，减少锁竞争`。
2. 调整业务逻辑 SQL 执行顺序， 避免 update/delete 长时间持有锁的 SQL 在事务前面。
3. 避免`大事务`，尽量将大事务拆成多个小事务来处理，小事务发生锁冲突的几率也更小。
4. 以`固定的顺序`访问表和行。比如两个更新数据的事务，事务 A 更新数据的顺序为 1，2;事务 B 更新数据的顺序为 2，1。这样更可能会造成死锁。
5. 在并发比较高的系统中，不要显式加锁，特别是是在事务里显式加锁。如 select … for update 语句，如果是在事务里`（运行了 start transaction 或设置了autocommit 等于0）`,那么就会锁定所查找到的记录。
6. 尽量按`主键/索引`去查找记录，范围查找增加了锁冲突的可能性，也不要利用数据库做一些额外额度计算工作。比如有的程序会用到 “select … where … order by rand();”这样的语句，由于类似这样的语句用不到索引，因此将导致整个表的数据都被锁住。
7. 优化 SQL 和表设计，减少同时占用太多资源的情况。比如说，`减少连接的表`，将复杂 SQL `分解`为多个简单的 SQL。



### InnoDB 如何手动锁表

LOCK TABLES table_name read local; 将当前表设置为只读，不能进行插入或更新操作。

UNLOCK TABLES;锁住表了，使用UNLOCK进行释放。



### InnoDB 引擎的行锁是通过加在什么上完成(或称实现) 的?

InnoDB 行锁是通过给索引上的索引项加锁来实现的，这一点 MySQL 与 Oracle 不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB 这 种行锁实现特点意味着:只有通过索引条件检索数据，InnoDB 才使用行级 锁，否则，InnoDB 将使用表锁!

------



## 九、MySQL 调优

> 日常工作中你是怎么优化SQL的？
>
> SQL优化的一般步骤是什么，怎么看执行计划（explain），如何理解其中各个字段的含义？
>
> 如何写sql能够有效的使用到复合索引？
>
> 一条sql执行过长的时间，你如何优化，从哪些方面入手？
>
> 什么是最左前缀原则？什么是最左匹配原则？

### 影响  mysql 的性能因素

- 业务需求对MySQL的影响(合适合度)

- 存储定位对MySQL的影响
  - 不适合放进MySQL的数据
    - 二进制多媒体数据
    - 流水队列数据
    - 超大文本数据
  - 需要放进缓存的数据
    - 系统各种配置及规则数据
    - 活跃用户的基本信息数据
    - 活跃用户的个性化定制信息数据
    - 准实时的统计信息数据
    - 其他一些访问频繁但变更较少的数据

- Schema设计对系统的性能影响
  - 尽量减少对数据库访问的请求
  - 尽量减少无用数据的查询请求

- 硬件环境对系统性能的影响



### 性能分析

#### MySQL Query Optimizer

1. MySQL 中有专门负责优化 SELECT 语句的优化器模块，主要功能：通过计算分析系统中收集到的统计信息，为客户端请求的 Query 提供他认为最优的执行计划（他认为最优的数据检索方式，但不见得是 DBA 认为是最优的，这部分最耗费时间）

2. 当客户端向 MySQL 请求一条 Query，命令解析器模块完成请求分类，区别出是 SELECT 并转发给 MySQL Query Optimizer 时，MySQL Query Optimizer 首先会对整条 Query 进行优化，处理掉一些常量表达式的预算，直接换算成常量值。并对 Query 中的查询条件进行简化和转换，如去掉一些无用或显而易见的条件、结构调整等。然后分析 Query 中的 Hint 信息（如果有），看显示 Hint 信息是否可以完全确定该 Query 的执行计划。如果没有 Hint 或 Hint 信息还不足以完全确定执行计划，则会读取所涉及对象的统计信息，根据 Query 进行写相应的计算分析，然后再得出最后的执行计划。

#### MySQL常见瓶颈

- CPU：CPU 在饱和的时候一般发生在数据装入内存或从磁盘上读取数据时候

- IO：磁盘 I/O 瓶颈发生在装入数据远大于内存容量的时候

- 服务器硬件的性能瓶颈：top，free，iostat 和 vmstat 来查看系统的性能状态

#### 性能下降SQL慢 执行时间长 等待时间长 原因分析

- 查询语句写的烂
- 索引失效（单值、复合）
- 关联查询太多 join（设计缺陷或不得已的需求）
- 服务器调优及各个参数设置（缓冲、线程数等）



#### MySQL常见性能分析手段

在优化 MySQL 时，通常需要对数据库进行分析，常见的分析手段有**慢查询日志**，**EXPLAIN 分析查询**，**profiling分析**以及**show命令查询系统状态及系统变量**，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能。

#####  性能瓶颈定位 

我们可以通过 show 命令查看 MySQL 状态及变量，找到系统的瓶颈：

```mysql
Mysql> show status ——显示状态信息（扩展show status like ‘XXX’）

Mysql> show variables ——显示系统变量（扩展show variables like ‘XXX’）

Mysql> show innodb status ——显示InnoDB存储引擎的状态

Mysql> show processlist ——查看当前SQL执行，包括执行状态、是否锁表等

Shell> mysqladmin variables -u username -p password——显示系统变量

Shell> mysqladmin extended-status -u username -p password——显示状态信息
```



##### Explain(执行计划)

是什么：使用 **Explain** 关键字可以模拟优化器执行 SQL 查询语句，从而知道 MySQL 是如何处理你的 SQL 语句的。分析你的查询语句或是表结构的性能瓶颈

能干吗：
- 表的读取顺序
- 数据读取操作的操作类型
- 哪些索引可以使用
- 哪些索引被实际使用
- 表之间的引用
- 每张表有多少行被优化器查询

怎么玩：

- Explain + SQL语句
- 执行计划包含的信息（如果有分区表的话还会有**partitions**）

![expalin](https://tva1.sinaimg.cn/large/007S8ZIlly1gf2hsjk9zcj30kq01adfn.jpg)

各字段解释

- <mark>**id**</mark>（select 查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序）

  - id 相同，执行顺序从上往下
  - id 全不同，如果是子查询，id 的序号会递增，id 值越大优先级越高，越先被执行
  - id 部分相同，执行顺序是先按照数字大的先执行，然后数字相同的按照从上往下的顺序执行

- <mark> **select_type**</mark>（查询类型，用于区别普通查询、联合查询、子查询等复杂查询）

  - **SIMPLE** ：简单的 select 查询，查询中不包含子查询或 UNION
  - **PRIMARY**：查询中若包含任何复杂的子部分，最外层查询被标记为 PRIMARY
  - **SUBQUERY**：在select或where列表中包含了子查询
  - **DERIVED**：在 from 列表中包含的子查询被标记为 DERIVED，MySQL会递归执行这些子查询，把结果放在临时表里
  - **UNION**：若第二个select出现在UNION之后，则被标记为UNION，若UNION包含在from子句的子查询中，外层select将被标记为DERIVED
  - **UNION RESULT**：从UNION表获取结果的select

- <mark> **table**</mark>（显示这一行的数据是关于哪张表的）

- <mark> **type**</mark>（显示查询使用了那种类型，从最好到最差依次排列	**system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL** ）

  - system：表只有一行记录（等于系统表），是 const 类型的特例，平时不会出现
  - const：表示通过索引一次就找到了，const 用于比较 primary key 或 unique 索引，因为只要匹配一行数据，所以很快，如将主键置于 where 列表中，mysql 就能将该查询转换为一个常量
  - eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常见于主键或唯一索引扫描
  - ref：非唯一性索引扫描，范围匹配某个单独值得所有行。本质上也是一种索引访问，他返回所有匹配某个单独值的行，然而，它可能也会找到多个符合条件的行，多以他应该属于查找和扫描的混合体
  - range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般就是在你的where语句中出现了between、<、>、in等的查询，这种范围扫描索引比全表扫描要好，因为它只需开始于索引的某一点，而结束于另一点，不用扫描全部索引
  - index：Full Index Scan，index于ALL区别为index类型只遍历索引树。通常比ALL快，因为索引文件通常比数据文件小。（**也就是说虽然all和index都是读全表，但index是从索引中读取的，而all是从硬盘中读的**）
  - ALL：Full Table Scan，将遍历全表找到匹配的行

  tip: 一般来说，得保证查询至少达到range级别，最好到达ref

- <mark> **possible_keys**</mark>（显示可能应用在这张表中的索引，一个或多个，查询涉及到的字段若存在索引，则该索引将被列出，但不一定被查询实际使用）

- <mark> **key**</mark> 

  - 实际使用的索引，如果为 NULL，则没有使用索引

  - **查询中若使用了覆盖索引，则该索引和查询的 select 字段重叠，仅出现在key列表中**

![explain-key](https://tva1.sinaimg.cn/large/007S8ZIlly1gf2hsty7iaj30nt0373yb.jpg)

- <mark> **key_len**</mark>

  - 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好
  - key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的

- <mark> **ref**</mark> （显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值）

- <mark> **rows**</mark> （根据表统计信息及索引选用情况，大致估算找到所需的记录所需要读取的行数）

- <mark> **Extra**</mark>（包含不适合在其他列中显示但十分重要的额外信息）

  1. <font color=red>using filesort</font>: 说明mysql会对数据使用一个外部的索引排序，不是按照表内的索引顺序进行读取。mysql中无法利用索引完成的排序操作称为“文件排序”。常见于order by和group by语句中

  2. <font color=red>Using temporary</font>：使用了临时表保存中间结果，mysql在对查询结果排序时使用临时表。常见于排序order by和分组查询group by。

  3. <font color=red>using index</font>：表示相应的select操作中使用了覆盖索引，避免访问了表的数据行，效率不错，如果同时出现using where，表明索引被用来执行索引键值的查找；否则索引被用来读取数据而非执行查找操作

  4. using where：使用了where过滤

  5. using join buffer：使用了连接缓存

  6. impossible where：where子句的值总是false，不能用来获取任何元祖

  7. select tables optimized away：在没有group by子句的情况下，基于索引优化操作或对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化

  8. distinct：优化distinct操作，在找到第一匹配的元祖后即停止找同样值的动作

     

**case**:

![explain-demo](https://tva1.sinaimg.cn/large/007S8ZIlly1gf2hszmc0lj30lc05w75c.jpg)

1. 第一行（执行顺序4）：id列为1，表示是union里的第一个select，select_type列的primary表示该查询为外层查询，table列被标记为\<derived3>，表示查询结果来自一个衍生表，其中derived3中3代表该查询衍生自第三个select查询，即id为3的select。【select d1.name......】

2. 第二行（执行顺序2）：id为3，是整个查询中第三个select的一部分。因查询包含在from中，所以为derived。【select id,name from t1 where other_column=''】
3. 第三行（执行顺序3）：select列表中的子查询select_type为subquery，为整个查询中的第二个select。【select id from t3】
4. 第四行（执行顺序1）：select_type为union，说明第四个select是union里的第二个select，最先执行【select name,id from t2】
5. 第五行（执行顺序5）：代表从union的临时表中读取行的阶段，table列的<union1,4>表示用第一个和第四个select的结果进行union操作。【两个结果union操作】



##### 慢查询日志

MySQL 的慢查询日志是 MySQL 提供的一种日志记录，它用来记录在 MySQL 中响应时间超过阈值的语句，具体指运行时间超过 `long_query_time` 值的 SQL，则会被记录到慢查询日志中。

- `long_query_time` 的默认值为10，意思是运行10秒以上的语句
- 默认情况下，MySQL数据库没有开启慢查询日志，需要手动设置参数开启

**查看开启状态**

```mysql
SHOW VARIABLES LIKE '%slow_query_log%'
```

**开启慢查询日志**

- 临时配置：

```mysql
mysql> set global slow_query_log='ON';
mysql> set global slow_query_log_file='/var/lib/mysql/hostname-slow.log';
mysql> set global long_query_time=2;
```

​	也可 set 文件位置，系统会默认给一个缺省文件 host_name-slow.log

​	使用 set 操作开启慢查询日志只对当前数据库生效，如果 MySQL 重启则会失效。

- 永久配置

  修改配置文件 my.cnf 或 my.ini，在[mysqld]一行下面加入两个配置参数

```mysql
[mysqld]
slow_query_log = ON
slow_query_log_file = /var/lib/mysql/hostname-slow.log
long_query_time = 3
```

注：log-slow-queries 参数为慢查询日志存放的位置，一般这个目录要有 MySQL 的运行帐号的可写权限，一般都将这个目录设置为 MySQL 的数据存放目录；long_query_time=2 中的 2 表示查询超过两秒才记录；在my.cnf或者 my.ini 中添加 log-queries-not-using-indexes 参数，表示记录下没有使用索引的查询。

可以用 `select sleep(4)` 验证是否成功开启。

在生产环境中，如果手工分析日志，查找、分析SQL，还是比较费劲的，所以MySQL提供了日志分析工具**mysqldumpslow**。

通过 mysqldumpslow --help 查看操作帮助信息

- 得到返回记录集最多的10个SQL

  `mysqldumpslow -s r -t 10 /var/lib/mysql/hostname-slow.log`

- 得到访问次数最多的10个SQL

  `mysqldumpslow -s c -t 10 /var/lib/mysql/hostname-slow.log`

- 得到按照时间排序的前10条里面含有左连接的查询语句

  `mysqldumpslow -s t -t 10 -g "left join" /var/lib/mysql/hostname-slow.log`

- 也可以和管道配合使用

  `mysqldumpslow -s r -t 10 /var/lib/mysql/hostname-slow.log | more`

**也可使用 pt-query-digest 分析 RDS MySQL 慢查询日志**



##### Show Profile 分析查询

通过慢日志查询可以知道哪些 SQL 语句执行效率低下，通过 explain 我们可以得知 SQL 语句的具体执行情况，索引使用等，还可以结合`Show Profile`命令查看执行状态。

- Show Profile 是 MySQL 提供可以用来分析当前会话中语句执行的资源消耗情况。可以用于  SQL 的调优的测量

- 默认情况下，参数处于关闭状态，并保存最近15次的运行结果

- 分析步骤

  1. 是否支持，看看当前的mysql版本是否支持

     ```mysql
     mysql>Show  variables like 'profiling';  --默认是关闭，使用前需要开启
     ```

  2. 开启功能，默认是关闭，使用前需要开启

     ```mysql
     mysql>set profiling=1;  
     ```

  3. 运行SQL

  4. 查看结果

     ```mysql
      mysql> show profiles;
     +----------+------------+---------------------------------+
      | Query_ID | Duration   | Query                           |
     +----------+------------+---------------------------------+
      |        1 | 0.00385450 | show variables like "profiling" |
     |        2 | 0.00170050 | show variables like "profiling" |
      |        3 | 0.00038025 | select * from t_base_user       |
     +----------+------------+---------------------------------+
     ```
  
  5. 诊断SQL，show profile cpu,block io for query  id(上一步前面的问题SQL数字号码)
  6. 日常开发需要注意的结论
  
     - converting HEAP to MyISAM 查询结果太大，内存都不够用了往磁盘上搬了。
  
     - create tmp table 创建临时表，这个要注意
  
     - Copying to tmp table on disk   把内存临时表复制到磁盘
  
     - locked



> 查询中哪些情况不会使用索引？

### 性能优化

#### 索引优化

1. 全值匹配我最爱

2. **最佳左前缀法则**，比如建立了一个联合索引(a,b,c)，那么其实我们可利用的索引就有(a), (a,b), (a,b,c)

3. 不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描

4. 存储引擎不能使用索引中范围条件右边的列

5. 尽量使用**覆盖索引**(只访问索引的查询(索引列和查询列一致))，减少select 

6. is null ,is not null 也无法使用索引

7. like "xxxx%" 是可以用到索引的，like "%xxxx" 则不行(like "%xxx%" 同理)。like以通配符开头('%abc...')索引失效会变成全表扫描的操作，

8. 字符串不加单引号索引失效

9. 少用or，用它来连接时会索引失效

10. <，<=，=，>，>=，BETWEEN，IN 可用到索引，<>，not in ，!= 则不行，会导致全表扫描

11. 前缀索引：前缀索引就是用某个字段中，字符串的前几个字符建立索引，比如我们可以在订单表上对商品名称字段的前 5 个字符建立索引。使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。

    但是，前缀索引有一定的局限性，例如 order by 就无法使用前缀索引，无法把前缀索引用作覆盖索引。



#### 建索引的几大原则

1. 最左前缀匹配原则，非常重要的原则，mysql 会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如 `a = 1 and b = 2 and c > 3 and d = 4` 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
2. =和in可以乱序，比如 `a = 1 and b = 2 and c = 3`  建立(a,b,c)索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式。
3. 尽量选择**区分度高**的列作为索引，区分度的公式是 `区分度 = distinct(col)/count(*)`，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。
4. 索引列不能参与计算，保持列“干净”，比如  `from_unixtime(create_time) = ’2014-05-29’`  就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成 `create_time = unix_timestamp(’2014-05-29’)`。
5. 尽量的扩展索引，不要新建索引。比如表中已经有 a 的索引，现在要加 (a,b) 的索引，那么只需要修改原来的索引即可。



**一般性建议**

- 对于单键索引，尽量选择针对当前 query 过滤性更好的索引

- 在选择组合索引的时候，当前 query 中过滤性最好的字段在索引字段顺序中，位置越靠前越好

- 在选择组合索引的时候，尽量选择可以能够包含当前 query 中的 where 字句中更多字段的索引

- 尽可能通过分析统计信息和调整 query 的写法来达到选择合适索引的目的

- 少用 Hint 强制索引

  

#### 查询优化

##### 为查询缓存优化你的查询

大多数的 MySQL 服务器都开启了查询缓存。这是提高性最有效的方法之 一，而且这是被 MySQL 的数据库引擎处理的。当有很多相同的查询被执行了多次的时候，这些查询结果会被放到一个缓存中，这样，后续的相同的查询就不用操作表而直接访问缓存结果了。

这里最主要的问题是，对于程序员来说，这个事情是很容易被忽略的。因为，我们某些查询语句会让 MySQL 不使用缓存。请看下面的示例:

![](https://tva1.sinaimg.cn/large/e6c9d24ely1h3b1ms39aaj218605u75x.jpg)

上面两条 SQL 语句的差别就是 CURDATE() ，MySQL 的查询缓存对这个函数不起作用。

所以，像 NOW() 和 RAND() 或是其它的诸如此类的 SQL 函数都不会开启查询缓存，因为这些函数的返回是会不定的易变的。所以，你所需要的就是 用一个变量来代替 MySQL 的函数，从而开启缓存。



##### 永远小标驱动大表（小的数据集驱动大的数据集）

```mysql
slect * from A where id in (select id from B)`等价于
#等价于
select id from B
select * from A where A.id=B.id
```

当 B 表的数据集必须小于 A 表的数据集时，用 in 优于 exists

```mysql
select * from A where exists (select 1 from B where B.id=A.id)
#等价于
select * from A
select * from B where B.id = A.id`
```

当 A 表的数据集小于 B 表的数据集时，用 exists 优于用 in

注意：A 表与 B 表的 ID 字段应建立索引。



##### order by 关键字优化

- order by 子句，尽量使用 Index 方式排序，避免使用 FileSort 方式排序

- MySQL 支持两种方式的排序，FileSort 和 Index，Index效率高，它指 MySQL 扫描索引本身完成排序，FileSort 效率较低；

  > https://blog.csdn.net/yangyu112654374/article/details/4251624  看个例子

- ORDER BY 满足两种情况，会使用 Index 方式排序；
  - ORDER BY 语句使用索引最左前列 
  -   使用 where 子句与 ORDER BY 子句条件列组合满足索引最左前列
  
- 尽可能在索引列上完成排序操作，遵照建索引的最左前缀

- 如果不在索引列上，filesort 有两种算法，mysql 就要启动双路排序和单路排序
  - 双路排序：MySQL 4.1之前是使用双路排序，字面意思就是两次扫描磁盘，最终得到数据
  - 单路排序：从磁盘读取查询需要的所有列，按照 order by 列在 buffer 对它们进行排序，然后扫描排序后的列表进行输出，效率高于双路排序
  
- 优化策略

  - 增大 sort_buffer_size 参数的设置
  - 增大 max_lencth_for_sort_data 参数的设置



##### GROUP BY 关键字优化

- group by 实质是先排序后进行分组，遵照索引建的最佳左前缀
- 当无法使用索引列，增大 `max_length_for_sort_data` 参数的设置，增大 `sort_buffer_size` 参数的设置
- where 高于 having，能写在 where 限定的条件就不要去 having 限定了



#### 数据类型优化

MySQL 支持的数据类型非常多，选择正确的数据类型对于获取高性能至关重要。不管存储哪种类型的数据，下面几个简单的原则都有助于做出更好的选择。

- 更小的通常更好：一般情况下，应该尽量使用可以正确存储数据的最小数据类型。

  简单就好：简单的数据类型通常需要更少的CPU周期。例如，整数比字符操作代价更低，因为字符集和校对规则（排序规则）使字符比较比整型比较复杂。

- 尽量避免NULL：通常情况下最好指定列为NOT NULL



### 说一下大表的优化方案

https://blog.csdn.net/u011516972/article/details/89098732



**一条sql执行过长的时间，你如何优化，从哪些方面？**

> 1、查看sql是否涉及多表的联表或者子查询，如果有，看是否能进行业务拆分，相关字段冗余或者合并成临时表（业务和算法的优化）
>
> 2、涉及连表的查询，是否能进行分表查询，单表查询之后的结果进行字段整合
>
> 3、如果以上两种都不能操作，非要连表查询，那么考虑对相对应的查询条件做索引。加快查询速度
>
> 4、针对数量大的表进行历史表分离（如交易流水表）
>
> 5、数据库主从分离，读写分离，降低读写针对同一表同时的压力，至于主从同步，mysql有自带的binlog实现 主从同步
>
> 6、explain分析sql语句，查看执行计划，分析索引是否用上，分析扫描行数等等
>
> 7、查看mysql执行日志，看看是否有其他方面的问题



------



## 十、分区、分表、分库

### MySQL分区

一般情况下我们创建的表对应一组存储文件，使用`MyISAM`存储引擎时是一个`.MYI`和`.MYD`文件，使用`Innodb`存储引擎时是一个`.ibd`和`.frm`（表结构）文件。

当数据量较大时（一般千万条记录级别以上），MySQL的性能就会开始下降，这时我们就需要将数据分散到多组存储文件，保证其单个文件的执行效率

**能干嘛**

- 逻辑数据分割
- 提高单一的写和读应用速度
- 提高分区范围读查询的速度
- 分割数据能够有多个不同的物理文件路径
- 高效的保存历史数据

**怎么玩**

首先查看当前数据库是否支持分区

- MySQL5.6以及之前版本：  

  ```mysql
  SHOW VARIABLES LIKE '%partition%';
  ```

- MySQL5.6：

  ```mysql
  show plugins;
  ```

**分区类型及操作**

- **RANGE分区**：基于属于一个给定连续区间的列值，把多行分配给分区。mysql将会根据指定的拆分策略，,把数据放在不同的表文件上。相当于在文件上,被拆成了小块.但是,对外给客户的感觉还是一张表，透明的。

  按照 range 来分，就是每个库一段连续的数据，这个一般是按比如**时间范围**来的，比如交易表啊，销售表啊等，可以根据年月来存放数据。可能会产生热点问题，大量的流量都打在最新的数据上了。

  range 来分，好处在于说，扩容的时候很简单。

- **LIST分区**：类似于按RANGE分区，每个分区必须明确定义。它们的主要区别在于，LIST分区中每个分区的定义和选择是基于某列的值从属于一个值列表集中的一个值，而RANGE分区是从属于一个连续区间值的集合。

- **HASH分区**：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL 中有效的、产生非负整数值的任何表达式。

  hash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表 

- **KEY分区**：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值。

**看上去分区表很帅气，为什么大部分互联网还是更多的选择自己分库分表来水平扩展咧？**

- 分区表，分区键设计不太灵活，如果不走分区键，很容易出现全表锁
- 一旦数据并发量上来，如果在分区表实施关联，就是一个灾难
- 自己分库分表，自己掌控业务场景与访问模式，可控。分区表，研发写了一个sql，都不确定mysql是怎么玩的，不太可控



> 随着业务的发展，业务越来越复杂，应用的模块越来越多，总的数据量很大，高并发读写操作均超过单个数据库服务器的处理能力怎么办？

这个时候就出现了**数据分片**，数据分片指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中。数据分片的有效手段就是对关系型数据库进行分库和分表。

区别于分区的是，分区一般都是放在单机里的，用的比较多的是时间范围分区，方便归档。只不过分库分表需要代码实现，分区则是mysql内部实现。分库分表和分区并不冲突，可以结合使用。



### 如何确定分库还是分表？

> 针对“如何确定分库还是分表？”的问题，你要结合具体的场景。

#### 何时分表

当数据量过大造成事务执行缓慢时，就要考虑分表，因为减少每次查询数据总量是解决数据查询缓慢的主要原因。你可能会问：“查询可以通过主从分离或缓存来解决，为什么还要分表？”但这里的查询是指事务中的查询和更新操作。

#### 何时分库

为了应对高并发，一个数据库实例撑不住，即单库的性能无法满足高并发的要求，就把并发请求分散到多个实例中去（这种应对高并发的思路我之前也说过）。

总的来说，分库分表使用的场景不一样： 

- **分表是因为数据量比较大，导致事务执行缓慢；**

- **分库是因为单库的性能无法满足要求。**



### MySQL分库

#### 为什么要分库?

数据库集群环境后都是多台 slave，基本满足了读取操作;  但是写入或者说大数据、频繁的写入操作对 master 性能影响就比较大，这个时候，单库并不能解决大规模并发写入的问题，所以就会考虑分库。

#### 分库是什么？   

一个库里表太多了，导致了海量数据，系统性能下降，把原本存储于一个库的表拆分存储到多个库上， 通常是将表按照功能模块、关系密切程度划分出来，部署到不同库上。

#### 分库的优点：

- 减少增量数据写入时的锁对查询的影响

- 由于单表数量下降，常见的查询操作由于减少了需要扫描的记录，使得单表单次查询所需的检索行数变少，减少了磁盘IO，时延变短

但是它无法解决单表数据量太大的问题



### MySQL分表

分表有两种分割方式，一种垂直拆分，另一种水平拆分。

- **垂直拆分**

  垂直分表，通常是按照业务功能的使用频次，把主要的、热门的字段放在一起做为主要表。然后把不常用的，按照各自的业务属性进行聚集，拆分到不同的次要表中；主要表和次要表的关系一般都是一对一的。

- **水平拆分(数据分片)**

  单表的容量不超过500W，否则建议水平拆分。是把一个表复制成同样表结构的不同表，然后把数据按照一定的规则划分，分别存储到这些表中，从而保证单表的容量不会太大，提升性能；当然这些结构一样的表，可以放在一个或多个数据库中。

  水平分割的几种方法：

  - 使用MD5哈希，做法是对UID进行md5加密，然后取前几位（我们这里取前两位），然后就可以将不同的UID哈希到不同的用户表（user_xx）中了。
  - 还可根据时间放入不同的表，比如：article_201601，article_201602。
  - 按热度拆分，高点击率的词条生成各自的一张表，低热度的词条都放在一张大表里，待低热度的词条达到一定的贴数后，再把低热度的表单独拆分成一张表。
  - 根据ID的值放入对应的表，第一个表user_0000，第二个100万的用户数据放在第二 个表user_0001中，随用户增加，直接添加用户表就行了。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1geuibkd9mjj31ns0u0aj1.jpg)





**分库分表后的难题**

分布式事务的问题，数据的完整性和一致性问题。

数据操作维度问题：用户、交易、订单各个不同的维度，用户查询维度、产品数据分析维度的不同对比分析角度。 跨库联合查询的问题，可能需要两次查询 跨节点的count、order by、group by以及聚合函数问题，可能需要分别在各个节点上得到结果后在应用程序端进行合并 额外的数据管理负担，如：访问数据表的导航定位 额外的数据运算压力，如：需要在多个节点执行，然后再合并计算程序编码开发难度提升，没有太好的框架解决，更多依赖业务看如何分，如何合，是个难题。



### 分布式ID生成方案

> 分库分表之后，id主键如何处理？
>
> 推荐：https://zhuanlan.zhihu.com/p/107939861

- UUID：`UUID`的生成简单到只有一行代码，输出结果 `c2b8c2b9e46c47e3b30dca3b0d447718`，但UUID却并不适用于实际的业务需求。像用作订单号`UUID`这样的字符串没有丝毫的意义，看不出和订单相关的有用信息；而对于数据库来说用作业务`主键ID`，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作`分布式ID`。

- 数据库自增ID：需要一个单独的MySQL实例用来生成ID（DB单点存在宕机风险，无法扛住高并发场景）

- 数据库多主模式

- 号段模式

- Redis：利用`redis`的 `incr`命令实现ID的原子性自增。

- 雪花算法（SnowFlake）：`Snowflake`生成的是Long类型的ID，一个Long类型占8个字节，每个字节占8比特，也就是说一个Long类型占64个比特

  Snowflake ID组成结构：`正数位`（占1比特）+ `时间戳`（占41比特）+ `机器ID`（占5比特）+ `数据中心`（占5比特）+ `自增值`（占12比特），总共64比特组成的一个Long类型。

- 滴滴出品（TinyID）

- 百度 （Uidgenerator）

- 美团（Leaf）



## 十一、主从复制

> 配主从，正经公司的话，也不会让 Javaer 去搞的，但还是要知道

### 复制的基本原理

- slave 会从 master 读取 binlog 来进行数据同步

- 三个步骤

  1. master将改变记录到二进制日志（binary log）。这些记录过程叫做二进制日志事件，binary log events；
  2. salve 将 master 的 binary log events 拷贝到它的中继日志（relay log）;
  3. slave 重做中继日志中的事件，将改变应用到自己的数据库中。MySQL 复制是异步且是串行化的。

  > 这么记忆
  >
  > - 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。
  >
  > - 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。
  >
  > - 回放 Binlog：回放 binlog，并更新存储数据
  
  ![](https://picbed-1302638964.cos.ap-beijing.myqcloud.com/mysql/94aec4abf353527cbbe2bef5a484471d.jpeg)



### MySQL 一主多从

一旦你提及“一主多从”，面试官很容易设陷阱问你：那大促流量大时，是不是只要多增加几台从库，就可以抗住大促的并发读请求了？

当然不是。

因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。

其实，你从 MySQL 主从复制过程也能发现，MySQL 默认是异步模式：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。

而这时，面试官一般会追问你“**MySQL 主从复制还有哪些模型？”**主要有三种。

- 同步复制：事务线程要等待所有从库的复制成功响应。

- 异步复制：事务线程完全不等待从库的复制成功响应。

- 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。

这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。



### 复制的基本原则

- 每个 slave只有一个 master
- 每个 salve只能有一个唯一的服务器 ID
- 每个master可以有多个salve



### 复制的最大问题

- 延时



### 从架构上解决主从复制延迟

- 使用数据冗余
- 使用缓存解决

------



## 十二f、其他问题

### 说一说三个范式

- 第一范式（1NF）：数据库表中的字段都是单一属性的，不可再分。这个单一属性由基本类型构成，包括整型、实数、字符型、逻辑型、日期型等。
- 第二范式（2NF）：数据库表中不存在非关键字段对任一候选关键字段的部分函数依赖，也即所有非关键字段都完全依赖于任意一组候选关键字。(数据库表中的非主属性只依赖于主键)
- 第三范式（3NF）：在第二范式的基础上，数据表中如果不存在非关键字段对任一候选关键字段的传递函数依赖则符合第三范式。所谓传递函数依赖，指的是如果存在"A → B → C"的决定关系，则C传递函数依赖于A。因此，满足第三范式的数据库表应该不存在如下依赖关系： 关键字段 → 非关键字段 x → 非关键字段y



### 百万级别或以上的数据如何删除

关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件，所以当我们对数据的增加、修改、删除，都会产生额外的对索引文件的操作，这些操作需要消耗额外的IO，会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。

1. 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）
2. 然后删除其中无用数据（此过程需要不到两分钟）
3. 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。
4. 与之前的直接删除绝对是要快速很多，更别说万一删除中断，一切删除会回滚。那更是坑了。



### limit 100000 加载很慢的话，你是怎么解决的呢？

在 mysql 中 limit 可以实现快速分页，但是如果数据到了几百万时我们的 limit 必须优化才能有效的合理的实现分页了，否则可能卡死你的服务器

**当一个表数据有几百万的数据的时候成了问题！**

日常分页SQL语句

```mysql
select id,name,content from users order by id asc limit 100000,20
```

扫描100020行

如果记录了上次的最大ID

```mysql
 select id,name,content from users where id>100073 order by id asc limit 20
```

扫描 20 行。 

总数据有500万左右，以下例子

```mysql
select * from wl_tagindex where byname='f' order by id limit 300000,10 
```

执行时间是 3.21s

优化后：

```mysql
select * from (
  select id from wl_tagindex
where byname='f' order by id limit 300000,10
) a
left join wl_tagindex b on a.id=b.id
```

  执行时间为 0.11s 速度明显提升

  这里需要说明的是 我这里用到的字段是 byname ,id 需要把这两个字段做复合索引，否则的话效果提升不明显



### 在高并发情况下，如何做到安全的修改同一行数据？

**1、使用悲观锁**

悲观锁本质是当前只有一个线程执行操作，排斥外部请求的修改。遇到加锁的状态，就必须等待。结束了唤醒其他线程进行处理。虽然此方案的确解决了数据安全的问题，但是，我们的场景是“高并发”。也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。

**2、FIFO（First Input First Output，先进先出）缓存队列思路**

直接将请求放入队列中，就不会导致某些请求永远获取不到锁。看到这里，是不是有点强行将多线程变成单线程的感觉哈。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190508231654761.)

然后，我们现在解决了锁的问题，全部请求采用“先进先出”的队列方式来处理。那么新的问题来了，高并发的场景下，因为请求很多，很可能一瞬间将队列内存“撑爆”，然后系统又陷入到了异常状态。或者设计一个极大的内存队列，也是一种方案，但是，系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。也就是说，队列内的请求会越积累越多，最终Web系统平均响应时间还是会大幅下降，系统还是陷入异常。

**3、使用乐观锁**

这个时候，我们就可以讨论一下“乐观锁”的思路了。乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。这样的话，我们就不需要考虑队列的问题，不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。



### 表中有大字段 **X**(例如:**text** 类型)，且字段 **X** 不会经常更新，以读为 为主，将该字段拆成子表好处是什么?

如果字段里面有大字段(text,blob)类型的，而且这些字段的访问并不多，这 时候放在一起就变成缺点了。 MYSQL 数据库的记录存储是按行存储的，数据 块大小又是固定的(16K)，每条记录越小，相同的块存储的记录就越多。此 时应该把大字段拆走，这样应付大部分小字段的查询时，就能提高效率。当需 要查询大字段时，此时的关联查询是不可避免的，但也是值得的。拆分开后， 对字段的 UPDAE 就要 UPDATE 多个表了







## 参考与感谢：

https://zhuanlan.zhihu.com/p/29150809

https://juejin.im/post/5e3eb616f265da570d734dcb#heading-105

https://blog.csdn.net/yin767833376/article/details/81511377









 